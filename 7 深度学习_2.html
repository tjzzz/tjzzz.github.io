<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  7 深度学习 - zhenzhen学习笔记
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="zhenzhen学习笔记" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site: ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_self" href="about_me.html">aboutme</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="http://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; zhenzhen学习笔记</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">Home</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_self" href="about_me.html">aboutme</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="1%20tools.html">1 tools</a></li>
        
            <li><a href="2%20get%20data.html">2 get data</a></li>
        
            <li><a href="3%20%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.html">3 数据可视化</a></li>
        
            <li><a href="4%20%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95.html">4 统计方法</a></li>
        
            <li><a href="5%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">5 机器学习</a></li>
        
            <li><a href="6%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.html">6 推荐系统</a></li>
        
            <li><a href="6%20%E6%96%87%E6%9C%AC&%E8%A7%86%E9%A2%91.html">6 文本&视频</a></li>
        
            <li><a href="7%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html">7 深度学习</a></li>
        
            <li><a href="10%20%E6%AF%94%E8%B5%9B%E5%AD%A6%E4%B9%A0.html">10 比赛学习</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6-%E6%B8%85%E5%8D%95.html">数据科学-清单</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="15637025346737.html">
                
                  <h1>第五章 机器学习</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>这部分主要是补充了机器学习的一席基础知识点，这里先略过。后期再进行相应的补充。具体包括</p>

<ul>
<li><p>容量/过拟合/欠拟合</p></li>
<li><p>估计/偏差/方差</p></li>
<li><p>最大似然估计</p></li>
<li><p>贝叶斯统计</p></li>
<li><p>监督学习算法</p></li>
<li><p>无监督学习算法</p>

<ul>
<li>PCA</li>
<li>聚类</li>
</ul></li>
<li><p>梯度下降</p></li>
</ul>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2019/7/21</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='7%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html'>7 深度学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15637037731128.html">
                
                  <h1>第八章 深度模型中的优化</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">8.1 优化与学习</h2>

<p>本章主要关注这一类特定的优化问题:寻找神经网络上的一组参数 θ，它能显 著地降低代价函数 J(θ)，该代价函数通常包括整个训练集上的性能评估和额外的正 则化项。</p>

<p>通常来说极其学习的算法目标大概可以写成，降低期望泛化误差。即<strong>风险</strong><br/>
<img src="media/15209459008411/15209462294325.jpg" alt=""/></p>

<p>其中p_data是真实的分布，实际中我们只能最小化<strong>经验风险</strong><br/>
<img src="media/15209459008411/15209463229280.jpg" alt=""/></p>

<h2 id="toc_1">8.2神经网络优化中的挑战</h2>

<ul>
<li>病态： hessain矩阵病态</li>
<li>局部最小值：对于非凸函数可能存在多个局部最小值。</li>
<li>。。。</li>
</ul>

<h2 id="toc_2">8.3基本算法</h2>

<p>梯度下降机器变种一般是机器学习中应用较多的优化算法。</p>

<h3 id="toc_3">(1)随机梯度下降SGD</h3>

<p><img src="media/15209459008411/15221677256921.jpg" alt=""/></p>

<h3 id="toc_4">（2）动量</h3>

<p>动量是比SGD速度更快的一种方法，SGD相当于只是考虑梯度的方向，而动量是在此基础上加上速度。</p>

<p><img src="media/15209459008411/15221680917826.jpg" alt=""/></p>

<h2 id="toc_5">8.4 参数初始化</h2>

<h2 id="toc_6">参考资料</h2>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2019/7/21</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='7%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html'>7 深度学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15637025345687.html">
                
                  <h1>第六章 深度前馈网络</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>前馈神经网络，也叫做<strong>多层感知机</strong>(MLP),其目标是近似某个函数\(f^*\)。前馈网络定义了一个映射函数\(y=f(x,\theta)\),然后学习参数\(\theta\)的值，从而近似函数\(f^*\)</p>

<p><strong>前馈</strong></p>

<p>因为在模型的输出和模型本身之间没有反馈链接。一些基本术语概念：</p>

<ul>
<li>输出层，隐藏层，</li>
</ul>

<h2 id="toc_0">6.1 例子：学习XOR</h2>

<p>【例】学习异或逻辑，即当两个二进制值中恰好有一个是1时候返回1，其他都返回0.</p>

<p>【解法1】可以考虑按照回归分析的思路，使用均方误差作为损失函数(实际中对于二进制数据建模时不适合用MSE)。\(f(x,w,b)=x^Tw+b\)<br/>
可以利用矩阵求解方程得到w=0,b=0.5， 线性模型得到的结果是任意一点都为0.5<br/>
==&gt; 主要问题，该问题其实不是一个线性问题</p>

<p>【解】另一种思路：<strong>使用一个模型来学习一个不同的特征空间，在这个空间上线性模型能够表示这个问题。</strong><br/>
<img src="media/15090214789463/15093791249803.jpg" alt=""/></p>

<p>构造一个非线性函数\(h\)，一般默认的推荐是<strong>整流线性单元</strong>ReLU:<br/>
\[g(z)=max(0, z)\]<br/>
所以整个网络是\(f(x)=w^Tmax(0, w^Tx+c)+b\)</p>

<h2 id="toc_1">6.2 基于梯度的学习</h2>

<p>很多神经网络的非线性导致代价函数变得非凸，所以一般神经网络的训练一般使用迭代的方法。<br/>
凸优化从任何一种初始参数都会收敛，而用于非凸损失函数的随机梯度下降没有这种收敛的保障，并且对参数的初始值比较敏感。</p>

<h3 id="toc_2">代价函数</h3>

<p>多数情况下，对于一个参数模型\(f(y|x,\theta)\)，</p>

<ul>
<li>可以简单的使用最大似然原理。即利用训练数据和模型预测之间的<strong>交叉熵</strong>作为代价函数。</li>
<li>有时候也会出现仅预测在给定x条件下y的某些统计量。</li>
</ul>

<p><strong>最大似然学习条件分布</strong></p>

<p>大多数的神经网络采用最大似然的训练方法。这意味着代价函数就是负的对数似然。它与训练数据</p>

<p>\[J(\theta) = -E log p_{model}(y|x)\]</p>

<p><strong>学习条件统计量</strong></p>

<p>这里好像有个和高统中类似的结论。比如<br/>
\[f^*=arg min_{f} E_{x,y~p}||y-f(x)||^2\]<br/>
可以得到\(f^*=E_{y~p(y|x)}(y)\)</p>

<h3 id="toc_3">输出单元</h3>

<p>其实代价函数的选择与输出单元是有一定关系的</p>

<ul>
<li>高斯输出分布的线性单元</li>
</ul>

<p>线性输出层一般用来产生高斯分布的均值。</p>

<ul>
<li>伯努利输出分布的sigmoid单元</li>
<li>多项分布输出的softmax单元</li>
</ul>

<p>对于想要表示具有n个离散取值的变量分布时候，一般可以用softmax函数。其是sigmoid函数在多分类问题上的推广。<br/>
\[softmax(z_j)=\frac{exp(z_i)}{\sum_j exp(z_j)}\]</p>

<h2 id="toc_4">6.3 隐藏单元</h2>

<p>隐藏层单元的选择没有一个固定的指导标准，一般默认选择<strong>整流线性单元</strong></p>

<p><strong>整流线性单元的一些扩展</strong></p>

<p>其扩展形式主要是基于\(z_i&lt;0\)时候使用一个非零的斜率\(\alpha_i\)</p>

<p>绝对值整流、渗漏整流线性单元，参数化整流线性单元。</p>

<p><strong>maxout单元</strong><br/>
maxout将单元将z划分成k个组，每个组上的取值为其最大值。</p>

<p><strong>sigmoid，tanh</strong></p>

<h2 id="toc_5">6.4网络结构设计</h2>

<p>网络结构设计主要是设计网络的<strong>深度</strong>和每一层的<strong>宽度</strong></p>

<p><strong>1. 万能近似性质</strong></p>

<p>万能近似定力：一个前馈神经网络,如果具有线性输出层和至少一个具有任何一种“挤压”性质的寄货函数的隐藏层。只要给予网络足够数量的隐藏单元，它可以以任意精度来近似任何一个有限维空间到另一个有限维空间的Borel可测函数。</p>

<p>万能定理从理论上说明无论我们想要学习什么函数，都有一个大的MLP可以进行这个。但是我们并不能保证算法能够学习到这个函数</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2019/7/21</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='7%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html'>7 深度学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15637037679680.html">
                
                  <h1>第四章 数值计算</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">1.数值精度</h2>

<p>这部分对于了解具体的算法并没影响，但是在进行底层库开发时候经常会遇到，可能会出现<strong>上溢</strong>或者<strong>下溢</strong></p>

<p>（1）病态条件</p>

<p>条件数：表示函数相对于输入的微小变化而变化的快慢程度。<br/>
比如矩阵求逆运算的条件数=最大和最小特征值的模的比</p>

<h2 id="toc_1">2.优化算法</h2>

<ul>
<li>梯度下降</li>
<li>牛顿法</li>
</ul>

<p>一般把需要最优化的函数称为<strong>目标函数</strong></p>

<p><strong>梯度下降</strong></p>

<p>对于一个多维输入函数f，梯度\(\triangledown_x f(x)\),其在u方向上的<strong>方向导数</strong>，相当于f在u方向上的斜率<br/>
\(f(x+\alpha u)\)关于\(\alpha\)的导数(在\(\alpha =0\)时取得)</p>

<p>\[\frac{\partial}{\partial \alpha}f(x+\alpha u)=u^T\triangledown_x f(x)\]</p>

<p>为了是f最小，希望找到f下降最快的方向，因此有<br/>
\[min u^T\triangledown_x f(x)\]<br/>
u取为单位向量，很明显是在u和f夹角180时候，也就是两个完全相反的时候，下降最快。<br/>
梯度下降： \(x&#39;=x-\epsilon \triangledown_x f(x)\)</p>

<p><strong>牛顿法</strong></p>

<p>Hessian矩阵，二阶导数<br/>
<img src="media/15080580137304/15082408605128.jpg" alt=""/></p>

<h2 id="toc_2">3.有限制的约束</h2>

<p>对于有限制的优化，有时候可以将约束条件转化到原始的优化函数中，这里主要介绍一下通用的方法：<strong>KKT方法</strong>(拉格朗日方法的推广，可以非等式约束)</p>

<pre><code>@@@
http://blog.csdn.net/mr_kktian/article/details/53750424
</code></pre>

<h2 id="toc_3">参考资料</h2>

<p>梯度下降法 扩展阅读： <a href="https://www.jiqizhixin.com/articles/2016-11-21-4">https://www.jiqizhixin.com/articles/2016-11-21-4</a><br/>
<a href="http://www.cnblogs.com/maybe2030/p/4751804.html">http://www.cnblogs.com/maybe2030/p/4751804.html</a></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2019/7/21</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='7%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html'>7 深度学习</a></span>
          				   
                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="7 深度学习_1.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <h1>zhenzhen学习笔记</h1>
                <div class="site-des"></div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="tjzzz.github.io" title="GitHub">GitHub</a>

  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="1%20tools.html"><strong>1 tools</strong></a>
        
            <a href="2%20get%20data.html"><strong>2 get data</strong></a>
        
            <a href="3%20%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>3 数据可视化</strong></a>
        
            <a href="4%20%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95.html"><strong>4 统计方法</strong></a>
        
            <a href="5%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>5 机器学习</strong></a>
        
            <a href="6%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.html"><strong>6 推荐系统</strong></a>
        
            <a href="6%20%E6%96%87%E6%9C%AC&%E8%A7%86%E9%A2%91.html"><strong>6 文本&视频</strong></a>
        
            <a href="7%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html"><strong>7 深度学习</strong></a>
        
            <a href="10%20%E6%AF%94%E8%B5%9B%E5%AD%A6%E4%B9%A0.html"><strong>10 比赛学习</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6-%E6%B8%85%E5%8D%95.html"><strong>数据科学-清单</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15637036455584.html">1 估计的置信度</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15637025344954.html">1 安装-mac</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15637025346225.html">2 不同设计之间有统计学差异吗</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15637036605080.html">3 样本量的确定</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15637037691936.html">3.1 特征处理—— 特征选择</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
