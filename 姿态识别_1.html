<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  姿态识别 - zhenzhen数据科学笔记
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="zhenzhen数据科学笔记" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site: ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="_self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        <li id=""><a target="_self" href="about_me.html">aboutme</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="https://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; zhenzhen数据科学笔记</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="_self" href="index.html">Home</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        
        <li><a target="_self" href="about_me.html">aboutme</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="kdd&%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B.html">kdd&异常检测</a></li>
        
            <li><a href="%E8%BD%A8%E8%BF%B9%E5%88%86%E6%9E%90.html">轨迹分析</a></li>
        
            <li><a href="1%20Tools.html">1 Tools</a></li>
        
            <li><a href="2%20Get%20Data.html">2 Get Data</a></li>
        
            <li><a href="3%20%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.html">3 数据可视化</a></li>
        
            <li><a href="4%20%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95.html">4 统计方法</a></li>
        
            <li><a href="5%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html">5 机器学习</a></li>
        
            <li><a href="6%20NLP.html">6 NLP</a></li>
        
            <li><a href="7%20CV.html">7 CV</a></li>
        
            <li><a href="8%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html">8 深度学习</a></li>
        
            <li><a href="9%20%E6%AF%94%E8%B5%9B%E5%AD%A6%E4%B9%A0.html">9 比赛学习</a></li>
        
            <li><a href="11%20%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84.html">11 平台架构</a></li>
        
            <li><a href="%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6-%E6%B8%85%E5%8D%95.html">数据科学-清单</a></li>
        
            <li><a href="%E5%85%B6%E4%BB%96.html">其他</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
	$(function(){
		$('#menu_item_index').addClass('is_active');
	});
</script>
<div class="row">
	<div class="large-8 medium-8 columns">
		<div class="markdown-body home-categories">
		
			<div class="article">
                <a class="clearlink" href="15906325298474.html">
                
                  <h1>【工具】MMlab—mmdetection</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<p>本来主要是看 mmskelton的，后来发现其依赖mmdetection，于是将这几个都看了下。</p>

<h3 id="toc_0">1. mmdetection</h3>

<p><strong>1 安装</strong><br/>
进行目标检测:<br/>
配置要求<br/>
Linux or macOS (Windows is not currently officially supported)<br/>
Python 3.6+<br/>
PyTorch 1.3+<br/>
CUDA 9.2+ (If you build PyTorch from source, CUDA 9.0 is also compatible)<br/>
GCC 5+<br/>
mmcv</p>

<p>安装步骤：</p>

<pre><code class="language-text"># 进入自己的conda环境
conda install pytorch torchvision -c pytorch   # 注意版本
git clone https://github.com/open-mmlab/mmdetection.git
cd mmdetection
#
pip install -r requirements/build.txt
pip install &quot;git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI&quot;
pip install -v -e .  # or &quot;python setup.py develop&quot;
</code></pre>

<p>注意：git commit id将在步骤d中写入版本号，例如0.6.0 + 2e7045c。该版本还将保存在经过训练的模型中。建议您每次从github提取一些更新时都运行步骤d。如果修改了C ++ / CUDA代码，则此步骤为强制性的。<br/>
按照上述说明，mmdetection将安装在dev模式下，对代码进行的任何本地修改都将生效，而无需重新安装它（除非您提交了一些提交并希望更新版本号）。<br/>
如果要使用opencv-python-headless而不是opencv-python，可以在安装MMCV之前先安装它。</p>

<p>方法二:docker</p>

<pre><code class="language-text">git clone https://github.com/open-mmlab/mmdetection.git
cd mmdetection
sudo docker build -t mmdetection docker/
</code></pre>

<p>注意:如果没有gpu，需要将Dockerfile中的 Force_cuda改为0</p>

<p><strong>2.测试demo</strong></p>

<pre><code class="language-text">- demo/inference_demo.ipynb
</code></pre>

<pre><code class="language-python">from mmdet.apis import init_detector, inference_detector, show_result_pyplot
config_file = &#39;configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py&#39;
checkpoint_file = &#39;checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth&#39;
## checkpoint_file = &#39;https://open-mmlab.s3.ap-northeast-2.amazonaws.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth&#39;
# 这里要注意config_file 和checkpoint_file的匹配
# test a single image
img = &#39;demo/demo.jpg&#39;
model = init_detector(config_file, checkpoint_file, device=&#39;cpu&#39;)
result = inference_detector(model, img)
show_result_pyplot(model, img, result)
</code></pre>

<p>解释：</p>

<ol>
<li>基本上主要分为这三步吧，init_detector， inference_detector， show_result_pyplot 可以再细看下show_result_pyplot中model的属性，怎么把结果和位置提取出来<code>mmdet/models/detectors/base.py</code></li>
<li>如果本机的环境不好用的话，可以在colab上测试，<a href="https://colab.research.google.com/github/open-mmlab/mmdetection/blob/master/demo/mmdet_inference_colab.ipynb#scrollTo=CSDS0SJBWa2l">https://colab.research.google.com/github/open-mmlab/mmdetection/blob/master/demo/mmdet_inference_colab.ipynb#scrollTo=CSDS0SJBWa2l</a></li>
<li>多数时候运行不成功，应该都是环境或者版本不匹配问题</li>
</ol>

<p><strong>3.应用</strong><br/>
code详解<br/>
(1) demo里用的是coco的分类<br/>
mmdet.models.detectors # 是基础的各种检测方法<br/>
mmdet.models.detectors.base.show_result中可以看到三步骤中二的返回结果是怎么解析的</p>

<p>result格式: list[array([xmin, ymax, xmax, ymin, 置信度])]</p>

<p>mmdet.core.evaluation.class_names 中包含不同数据集定义的get_classes具体的类别</p>

<p>(2)替换自己的数据分类</p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2020/5/28</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E5%A7%BF%E6%80%81%E8%AF%86%E5%88%AB.html'>姿态识别</a></span>
          				   
                    

                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15899772943822.html">
                
                  <h1>数据集 & 工具</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<h2 id="toc_0">1.数据集-动态视频</h2>

<table>
<thead>
<tr>
<th>数据</th>
<th>基本情况</th>
<th>链接</th>
</tr>
</thead>

<tbody>
<tr>
<td>HMDB-51</td>
<td>51个类别，6766个视频</td>
<td><a href="https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/#dataset">https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/#dataset</a></td>
</tr>
<tr>
<td>UCF-101</td>
<td>101类别，13320个短视频</td>
<td><a href="https://www.crcv.ucf.edu/research/data-sets/ucf101/">https://www.crcv.ucf.edu/research/data-sets/ucf101/</a></td>
</tr>
<tr>
<td>Kinetics-700</td>
<td>650000个，用于ActivityNet比赛</td>
<td><a href="https://deepmind.com/research/open-source/open-source-datasets/kinetics/">https://deepmind.com/research/open-source/open-source-datasets/kinetics/</a></td>
</tr>
<tr>
<td>ava</td>
<td>google。多人不同动作数据集</td>
<td><a href="https://research.google.com/ava/explore.html">https://research.google.com/ava/explore.html</a></td>
</tr>
<tr>
<td>ActivityNet</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

<p>HMDB-51: 压缩包大概6G，主要包含类别</p>

<ul>
<li>常见面部动作（smile，laugh，chew，talk）</li>
<li>复杂面部动作</li>
<li>直梯动作</li>
<li>多人交互</li>
</ul>

<p><img src="media/15899772943822/15913243160558.jpg" alt="" style="width:971px;"/><br/>
<img src="media/15899772943822/15913243437161.jpg" alt="" style="width:1001px;"/></p>

<p>UCF-101: 包含的数据类型</p>

<ul>
<li>人物交互</li>
<li>人体动作</li>
<li>人人交互</li>
<li>乐器演奏</li>
<li>体育运动</li>
</ul>

<p><strong>ava-action：</strong><br/>
包含了80个原子动作(比如走路，握手。。) 数据包含三部分:</p>

<ul>
<li>person pose：</li>
<li>人-物交互</li>
<li>人人交互
<a href="https://research.google.com/ava/explore.html">https://research.google.com/ava/explore.html</a>
一共430个video，其中235个train， 64个validation，131 test。 每个视频是15min，每1s钟进行以此标注。<br/>
有一个机遇faster-rcnn的预训练模型。 tensorflow的目标检测api <a href="https://github.com/tensorflow/models/tree/master/research/object_detection#tensorflow-object-detection-api">https://github.com/tensorflow/models/tree/master/research/object_detection#tensorflow-object-detection-api</a></li>
</ul>

<p>数据格式 ava_train_v2.2.csv<br/>
video_id, middle_frame_timestamp, person_box, action_id, person_id</p>

<p>toread<br/>
<a href="https://www.jianshu.com/p/a4cc71126796">https://www.jianshu.com/p/a4cc71126796</a></p>

<h2 id="toc_1">2.数据集-静态图像</h2>

<p><a href="https://blog.csdn.net/lgk1996/article/details/79814888">https://blog.csdn.net/lgk1996/article/details/79814888</a></p>

<table>
<thead>
<tr>
<th>data</th>
<th>basic description</th>
<th>link</th>
</tr>
</thead>

<tbody>
<tr>
<td>Stanford40 Dataset</td>
<td>斯坦福-李飞飞团队穿件，包括40中不同的行为类别，9532张图片</td>
<td><a href="http://vision.stanford.edu/Datasets/40actions.html">http://vision.stanford.edu/Datasets/40actions.html</a></td>
</tr>
<tr>
<td>MPII Pose Dataset</td>
<td>主要是human pose estimation</td>
<td></td>
</tr>
</tbody>
</table>

<p><strong>Stanford40 Dataset</strong>: runing, phoning, play,drinking等, taking photos<br/>
B. Yao, X. Jiang, A. Khosla, A.L. Lin, L.J. Guibas, and L. Fei-Fei. Human Action Recognition by Learning Bases of Action Attributes and Parts. Internation Conference on Computer Vision (ICCV), Barcelona, Spain. November 6-13, 2011. </p>

<p><strong>MPII Pose Dataset</strong><br/>
<img src="media/15899772943822/15917594608821.jpg" alt=""/></p>

<h2 id="toc_2">3.工具</h2>

<p>常用的工具：<br/>
<a href="https://zhuanlan.zhihu.com/p/38597956">https://zhuanlan.zhihu.com/p/38597956</a></p>

<table>
<thead>
<tr>
<th>tool</th>
<th>说明</th>
<th>语言</th>
<th>链接</th>
</tr>
</thead>

<tbody>
<tr>
<td>openpose</td>
<td>多人，2D，实时</td>
<td>python-API</td>
<td><a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">https://github.com/CMU-Perceptual-Computing-Lab/openpose</a></td>
</tr>
<tr>
<td>facebook-densePose</td>
<td>mask-RCNN的变体</td>
<td></td>
<td>github.com/facebookresearch/Densepose</td>
</tr>
<tr>
<td>alphaPose</td>
<td></td>
<td>tf, pytorch</td>
<td></td>
</tr>
<tr>
<td>Human Body Pose Estimation</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Deeppose(2014)</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>

<h3 id="toc_3">(1)Open-mmlab家族</h3>

<p><strong>MMSkeleton</strong><br/>
原理的名字叫 st-gcn，后来改为 MMSkeleton<br/>
st-gcn: 比较老了 <a href="https://hub.docker.com/r/jaehwankimneo/st-gcn">https://hub.docker.com/r/jaehwankimneo/st-gcn</a><br/>
mmskelton: <a href="https://hub.docker.com/r/ioir123ju/mmskeleton/tags">https://hub.docker.com/r/ioir123ju/mmskeleton/tags</a></p>

<pre><code class="language-text">python setup.py develop
</code></pre>

<p>中间可能会报很多错，需要一步步的改<br/>
环境配置<a href="http://blog.sina.com.cn/s/blog_679f93560102wpyf.html">http://blog.sina.com.cn/s/blog_679f93560102wpyf.html</a></p>

<p>安装好mmskelton后，可以选安装mmdetection，这个参照其github上的说明来就行</p>

<p><strong>mmcv</strong></p>

<p>mmcv是一个MMLAB开发的cv的基础工具库，用来支持mmdetection，mmaction等</p>

<h3 id="toc_4">(2)openpose</h3>

<p>运行方式:</p>

<ol>
<li>命令行: ./build/examples/openpose/openpose.bin xxx</li>
<li>c++接口</li>
<li>Python接口</li>
</ol>

<p>docker: <code>docker pull exsidius/openpose</code></p>

<p>还有在google colab上进行尝试<a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/949#issue-387855863">https://github.com/CMU-Perceptual-Computing-Lab/openpose/issues/949#issue-387855863</a></p>

<pre><code class="language-text">! apt update
! apt install -y cmake sudo libopencv-dev
! git clone https://github.com/CMU-Perceptual-Computing-Lab/openpose.git
! cd openpose/ubuntu &amp;&amp; ./install_cmake.sh &amp;&amp; ./install_cuda.sh &amp;&amp; ./install_cudnn.sh
! cd openpose &amp;&amp; git pull origin master &amp;&amp; rm -r build || true &amp;&amp; mkdir build &amp;&amp; cd build &amp;&amp; cmake .. &amp;&amp; make -j`nproc`

# example demo usage
!cd openpose &amp;&amp; ./build/examples/openpose/openpose.bin --video examples/media/video.avi --write_json output/ --display 0 --render_pose 0
</code></pre>

<p>cpu-only的版本：要求8G内存</p>

<p>安装</p>

<pre><code class="language-text">git clone https://github.com/CMU-Perceptual-Computing-Lab/openpose
cd build/
make -j`nproc`
</code></pre>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2020/5/20</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E5%A7%BF%E6%80%81%E8%AF%86%E5%88%AB.html'>姿态识别</a></span>
          				   
                    

                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
			<div class="article">
                <a class="clearlink" href="15895350398690.html">
                
                  <h1>综述_action recognition</h1>
                  <div class="a-content">
                      
                      <div class="a-content-text">
                        
                        	<ul>
<li>
<a href="#toc_0">2 姿态识别方法</a>
<ul>
<li>
<a href="#toc_1">1. 传统的有监督的特征提取方法</a>
</li>
<li>
<a href="#toc_2">2.深度学习方法的探索</a>
</li>
<li>
<a href="#toc_3">3.深度学习方法</a>
</li>
</ul>
</li>
<li>
<a href="#toc_4">论文资料</a>
</li>
<li>
<a href="#toc_5">toread</a>
</li>
</ul>


<p>综述<br/>
<a href="https://mp.weixin.qq.com/s?__biz=MzU5MzQyMzk5OQ==&amp;mid=2247484885&amp;idx=1&amp;sn=09aaafd71959a9c0db703993f8386bd5&amp;chksm=fe11fec5c96677d3580e5f9ee7c9c4c5f4c31257442625afa5ae59e2f2954940e3c3a50acc33&amp;mpshare=1&amp;scene=1&amp;srcid=#rd">https://mp.weixin.qq.com/s?__biz=MzU5MzQyMzk5OQ==&amp;mid=2247484885&amp;idx=1&amp;sn=09aaafd71959a9c0db703993f8386bd5&amp;chksm=fe11fec5c96677d3580e5f9ee7c9c4c5f4c31257442625afa5ae59e2f2954940e3c3a50acc33&amp;mpshare=1&amp;scene=1&amp;srcid=#rd</a><br/>
<img src="media/15895350398690/15895361626225.jpg" alt="" style="width:603px;"/></p>

<p><a href="https://zhuanlan.zhihu.com/p/38241179">https://zhuanlan.zhihu.com/p/38241179</a></p>

<p>github: <a href="https://github.com/facebookresearch/DensePose">https://github.com/facebookresearch/DensePose</a></p>

<p>姿态识别</p>

<ul>
<li>动作识别： <a href="https://zhuanlan.zhihu.com/p/132673525">https://zhuanlan.zhihu.com/p/132673525</a></li>
<li>身份识别</li>
</ul>

<p>从识别的问题类型上划分的话，主要有这么几类问题</p>

<ul>
<li>手势识别</li>
<li>action: 短时间的动作识别</li>
<li>activity：持续时间较长的行为，比如读书，打电话，打球</li>
</ul>

<h2 id="toc_0">2 姿态识别方法</h2>

<p>参考：<br/>
<a href="https://zhuanlan.zhihu.com/p/79521655">https://zhuanlan.zhihu.com/p/79521655</a><br/>
<a href="https://zhuanlan.zhihu.com/p/103566134">https://zhuanlan.zhihu.com/p/103566134</a>  通用行为识别</p>

<h3 id="toc_1">1. 传统的有监督的特征提取方法</h3>

<blockquote>
<p>在深度学习之前，传统的特征的方法的SOTA是iDT 算法</p>
</blockquote>

<p>(1) 时空关键点<br/>
即检测视频图形中在时空维度上发生剧烈变化的数据，比如运动的轨迹。常见的方法</p>

<ul>
<li>Harris角点检测方法<br/>
详细介绍参考<a href="https://blog.csdn.net/lwzkiller/article/details/54633670">https://blog.csdn.net/lwzkiller/article/details/54633670</a> 简单地说：</li>
</ul>

<p>角点：一般是轮廓之间的交点，视角发生变化时候通常具备比较稳定的性质，该点附近区域的像素点无论在梯度方向上还是其梯度幅值上有着较大变化；<br/>
<img src="media/15895350398690/15899787701890.jpg" alt=""/><br/>
u,v是窗口的偏移量，w(x,y)是窗口函数作为权重<br/>
算法的基本思想：找到在任意方向上滑动，偏移较大的位置。</p>

<p>然后统计关键点周围的梯度直方图等视觉特征，</p>

<p>(2)密集轨迹iDT算法(improved Dense Trajectories)<br/>
《Dense Trajectories and Motion Boundary Descriptors for Action Recognition》和《Action Recognition with Improved Trajectories》<br/>
光流： <br/>
HOG：灰度图像梯度的直方图<br/>
HOF：光流的直方图<br/>
MBH：光流梯度的直方图</p>

<h3 id="toc_2">2.深度学习方法的探索</h3>

<p>因为行为识别需要考虑空间和时间两个维度，所以一开始主要是怎么将时间维度融合进去。比较有突破性的进展有:<br/>
<strong>(1) Single Stream Network</strong><br/>
paper: 2014_Large-scale Video Classification with Convolutional Neural Networks</p>

<p>使用数据：sports-1M,主要是关于人运动的一些video; 后来通过迁移学习将top1和top3layer重训应用到ucf101</p>

<ul>
<li>模型结构</li>
</ul>

<p><img src="media/15895350398690/15917649030549.jpg" alt=""/><br/>
single frame: 单线提取特征，最后再将则合格特征融合在一起<br/>
early fusion:将相邻的T个帧的信息合并，在第一层filter的时候进行修改，将filter改为f<em>f</em>3*T的<br/>
late fusion: 建立两个single frame，之间共享参数，在第一个全连接层的时候进行融合<br/>
slow fusion：有点将前两种方式合并的感觉，如图示</p>

<ul>
<li>效率问题<br/>
为了解决效率问题，在具体处理的时候，是将原本的一个网路结构拆分成两个:
<ul>
<li>context stream: 对原始图像做downsample. n/2</li>
<li>fovea stream。 只保留图像中间的部分(这个也有本身的原图像有关)
<img src="media/15895350398690/15917669610603.jpg" alt=""/></li>
</ul></li>
</ul>

<p>这样整个的参数减少为原来的1/2</p>

<ul>
<li>效果</li>
</ul>

<p>模型本身中slow fusion版本最好，整体上要比hand-crafted-feature的结果差。主要原因: 对于运动特征的捕捉不够；数据的丰富性问题</p>

<p><strong>(2)two stream network</strong><br/>
paper: Two-Stream Convolutional Networks for Action Recognition in Videos</p>

<ul>
<li>model
<img src="media/15895350398690/15912742720913.jpg" alt=""/></li>
</ul>

<p>包含两个网络分别处理空间和时间维度:<br/>
(1)Spatial Net 主要是提取视频的每一帧，其实就是典型的图像分类，可以用图像分类相关的网络结构<br/>
(2) temporal net<br/>
将提取的光流信息作为输入</p>

<ul>
<li><p>multi-task learning<br/>
Spatial Net可以用一些预训练的模型，temporal net的输入得是视频。作者用的训练数据集是ucf101和hmdb51，分别有9.5k和3.7k。为了减少过拟合，需要将数据合并成一个。</p></li>
<li><p>结果</p></li>
</ul>

<p><img src="media/15895350398690/15917755022444.jpg" alt=""/></p>

<h3 id="toc_3">3.深度学习方法</h3>

<p><strong>(1) LRCN</strong><br/>
paper: Long-term Recurrent Convolutional Networks for Visual Recognition and Description<br/>
code: <a href="https://github.com/garythung/torch-lrcn">https://github.com/garythung/torch-lrcn</a></p>

<p>基本思想： 1）不再使用传统的光流方法，而是引入RNN结构；（2）将编码解码结构拓展到视频表征领域；（3）为动作识别提出了一个端到端的训练结构。<br/>
<img src="media/15895350398690/15917767750263.jpg" alt=""/></p>

<p>如上图所示: 设计的是一种编码-解码 网络，通过cnn进行编码，然后rnn进行解码。</p>

<p><strong>(2) C3D</strong></p>

<p>code: <a href="https://github.com/hx173149/C3D-tensorflow">https://github.com/hx173149/C3D-tensorflow</a><br/>
paper: 2014 Learning Spatiotemporal Features with 3D Convolutional Networks<br/>
中文翻译 <a href="https://www.jianshu.com/p/09d1d8ffe8a4">https://www.jianshu.com/p/09d1d8ffe8a4</a></p>

<p>不同于前面将时间维度融合进去原有的模型中，或者是拆分成一个2d的图像+时间维度，3d卷积直接从3d的角度出发。3d卷积与2d卷积的区别:<br/>
<img src="media/15895350398690/15917785441863.jpg" alt=""/><br/>
a)在一个图像上应用2D卷积会产生一个图像。b)在视频卷上应用2D卷积(多个帧作为多个通道)也会产生一个图像。c)在视频卷上应用3D卷积可产生另一个卷，保留输入信号的时间信息。</p>

<p>3d卷积网络。本文中作者的主要结果是:</p>

<ul>
<li>3d卷积结构普遍比2d卷积结构要好</li>
<li>找到了一个表现最好的3x3x3的卷积核，这个特征提取器后来被广泛应用于action detection的基础</li>
</ul>

<p>网络结构：C3D的网络结构：8个卷积、5个池化、2个全连接，最后是一个softmax输出。所有3D卷积的尺寸都是3<em>3</em>3、步长为1<br/>
<img src="media/15895350398690/15917800540105.jpg" alt=""/></p>

<p>实验：</p>

<ul>
<li>卷积核的深度3x3xdi。 不同层均匀深度、不同深度。 通过实验发现3x3x3效果最好</li>
<li>作者使用sports-1M数据进行训练，每16帧作为一个片段，16</li>
<li>在UCF101上的效果</li>
</ul>

<p><img src="media/15895350398690/15917811110276.jpg" alt=""/></p>

<p><strong>TSF</strong><br/>
two stream fusion, 这个主要是基于two stream，摸底不同的融合方法。 其中作者之一就是前面提出two stream network的Andrew Zisserman<br/>
链接: <a href="http://www.robots.ox.ac.uk/%7Evgg/software/two_stream_action/">http://www.robots.ox.ac.uk/~vgg/software/two_stream_action/</a><br/>
code: matlab</p>

<p>作者提出视频行为识别的主要难点:1)训练数据量较少， 2)temporal信息提取的不够充分。<br/>
接着之前的two stream network的一些问题 1) 融合只在最后一步classification score了 2)</p>

<p>实验的控制因素：<br/>
（1）如何将空间和时间融合<br/>
可以简单理解之前分两路相当于是认为时间和空间这两个维度独立，然而实际上他们是有交互影响的，所以更好的方法是在layer的时候就能将信息融合。<br/>
作者列举的方法有: sum, max, Concatenation, Conv(Concatenation后再做个变换)，在操作的时候需要将两者的维度转成一样的。而不同的组合方法也会显著影响模型的参数数量。<br/>
(2) 在哪个layer进行融合<br/>
(3) 时间维度怎么融合</p>

<p><img src="media/15895350398690/15917935303561.jpg" alt=""/><br/>
作者提出的改进的网络结构如上图，每一部分都有空间(蓝色)和时间(绿色)维度的融合，<br/>
另一个问题是融合的时间间隔</p>

<p><strong>TSN</strong></p>

<p><strong>I3D</strong></p>

<p>slowonly</p>

<p><strong>slowfast</strong></p>

<p>CSN</p>

<p>SSN</p>

<p>一个无监督视频特征提取： <br/>
Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</p>

<h2 id="toc_4">论文资料</h2>

<p><a href="https://github.com/jinwchoi/awesome-action-recognition">https://github.com/jinwchoi/awesome-action-recognition</a><br/>
<a href="https://github.com/cbsudux/awesome-human-pose-estimation">https://github.com/cbsudux/awesome-human-pose-estimation</a></p>

<h2 id="toc_5">toread</h2>

<p><a href="https://www.jianshu.com/p/39fe654ed410">https://www.jianshu.com/p/39fe654ed410</a></p>

<p>[含代码]<a href="https://blog.csdn.net/DaGongJiGuoMaLu09/article/details/94628591">https://blog.csdn.net/DaGongJiGuoMaLu09/article/details/94628591</a></p>

                        
                      </div>
                  </div>
                </a>
                <div class="read-more clearfix">
                  <div class="more-left left">
                  
                    <span class="date">2020/5/15</span>
                    <span>posted in&nbsp;</span> 
          				  
          					    <span class="posted-in"><a href='%E5%A7%BF%E6%80%81%E8%AF%86%E5%88%AB.html'>姿态识别</a></span>
          				   
                    

                  </div>
                  <div class="more-right right">
                  <span class="comments">
                      

                       
                  </span>
                  </div>
                </div>
              </div><!-- article -->
        
              


			<div class="row">
			  <div class="large-6 columns">
			  <p class="text-left" style="padding-top:25px;">
			   <a href="姿态识别.html">&laquo; Prev Page</a>  
			  </p>
			  </div>
			  <div class="large-6 columns">
			<p class="text-right" style="padding-top:25px;">
			
			</p>
			  </div>
			</div>
		</div>
	</div><!-- large 8 -->

 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <h1>zhenzhen数据科学笔记</h1>
                <div class="site-des"></div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="tjzzz.github.io" title="GitHub">GitHub</a>

  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="kdd&%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B.html"><strong>kdd&异常检测</strong></a>
        
            <a href="%E8%BD%A8%E8%BF%B9%E5%88%86%E6%9E%90.html"><strong>轨迹分析</strong></a>
        
            <a href="1%20Tools.html"><strong>1 Tools</strong></a>
        
            <a href="2%20Get%20Data.html"><strong>2 Get Data</strong></a>
        
            <a href="3%20%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>3 数据可视化</strong></a>
        
            <a href="4%20%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95.html"><strong>4 统计方法</strong></a>
        
            <a href="5%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>5 机器学习</strong></a>
        
            <a href="6%20NLP.html"><strong>6 NLP</strong></a>
        
            <a href="7%20CV.html"><strong>7 CV</strong></a>
        
            <a href="8%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html"><strong>8 深度学习</strong></a>
        
            <a href="9%20%E6%AF%94%E8%B5%9B%E5%AD%A6%E4%B9%A0.html"><strong>9 比赛学习</strong></a>
        
            <a href="11%20%E5%B9%B3%E5%8F%B0%E6%9E%B6%E6%9E%84.html"><strong>11 平台架构</strong></a>
        
            <a href="%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6-%E6%B8%85%E5%8D%95.html"><strong>数据科学-清单</strong></a>
        
            <a href="%E5%85%B6%E4%BB%96.html"><strong>其他</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15917706614562.html">optical flow 光流</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15917693533998.html">process</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15917597006543.html">业务</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15917572981552.html">综述_关键点检测</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15916704949564.html">【工具】MMlab——mmaction</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>


  </body>
</html>
