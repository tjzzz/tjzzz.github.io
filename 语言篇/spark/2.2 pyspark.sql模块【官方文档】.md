# 2.2 pyspark.sql模块【官方文档】


## pyspark.sql.SparkSession
SparkSession是对dataframe和dataset数据进行操作的一个入口，常用的方式是

```python
from pyspark.sql import SparkSession
spark = SparkSession.builder \
        .master("local") \
        .appName("Word Count") \
        .config("spark.some.config.option", "some-value") \
        .getOrCreate()
```
bulider建立一个sparksession的实例，Builder有很多的属性

* master：设置spark master。可以有的选择有
    * local，local[4] 表示run locally with 4 cores
    * "spark://master:7077"  run on a Spark standalone cluster
* appName： 任务名称
* config是设置一些配置参数
    * 比如内存
    * 另外一种方式是已经有了一个conf对象，这时可以写成config(conf = conf)

    ```python
    from pyspark import SparkConf
    conf = SparkConf()
    conf = conf.set("spark.executor.memory", "10g")
    conf = conf.set("spark.driver.memory", "10g")
    conf = conf.set("spark.excutor.maxResultSize", "10g")
    conf = conf.set("spark.driver.maxResultSize", "10g")
    #
    spark = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()
    ```
    
* enableHiveSupport()   设置支持hive相关的操作
* getOrCreate()   如果当前没有SparkSession的话就创建一个，如果有的话就用当前的这个
    


## pyspark.sql.createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)

data：rdd，list， 或者是pandas.DataFrame创建一个spark的DataFrame
schema 表示的是列名，不指定的情况下spark会根据数据自己进行推断
samplingRatio是用来用于指定推断schema的数据的行数，如果不指定的话默认根据第一行的数据来判断

dataframe 相关的操作

```python
spark_df = spark.createDataFrame(pandas_df)
spark.range(start, end, step=1)   # 创建一个dataframe，只有一列id。元素是range的数据
spark.sql(sql_cmd)    # 返回一个Dataframe
spark.table(table_name)    # 将表数据返回为一个Dataframe
```


## 其他入库量

sparksession在spark2.0中是作为统一的入口。在1.0版本中会有不同的读入方式。

pyspark.sql.SQLContext()， HiveContext()


## pyspark.sql.DataFrame
 A distributed collection of data grouped into named columns.
 
 spark的dataframe的相关操作函数
 
 https://www.cnblogs.com/rxingyue/p/7113235.html
 
**agg**
对整个dataframe进行聚合(不分组)，比如求最小，最大值

```
from pyspark.sql import functions as F
df1.agg(F.mean(df1.duration)).collect()
```

**alias**

重命名

一些类似sql的函数功能
```
df.filter()     等价于 df.where()
groupby()
df1.join(df2, on=xx, how= xx)
df.select()
df.selectExpr(*expr)
df.limit()
df.orderby(*cols, **kwargs)
df.withColumn()   # 增加一个新列或者覆盖原有的列
```
其他

```
df.withColumnRenamed('old_name', 'new_name')     # 给某一列重新命名
df.printSchema().   #Prints out the schema in the tree format.
df.approxQuantile(col, prob)     # 近似的分位点
df.coalesce(n)                            # 返回一个新的按照number=n进行partition的dataframe
df.collect()                             # 将所有记录按照a list of Row返回 
df.count()            #返回行数
df.createGlobalTempView(table_name)    # 根据df创建一个临时的表，可以执行sql操作
df.createTempView(name)
df.crosstab(col1, col2)
df.describe(*cols)    #类似R的summary
df.summary()
## 删除某些列
df.drop(*cols)     # 去掉一些列
df.drop_duplicates(subset=None)
df.dropna(how=)
df.fillna(value, subset=None)    # 是df.na.fill()的缩写
foreach(f)    # Applies the f function to all Row of this DataFrame 对每行进行f函数操作

## 
df.repartition(num)
df.sample(withReplacement=None, fraction=None, seed=None)
##
df.toJSON()
df.toPandas().  # 将spark.sql的dataframe转为pandas的dataframe
```

## pyspark.sql.GroupedData

针对DataFrame.groupBy()之后可以做一些聚合操作。

agg()

* 常用的一些汇聚函数： avg/mean, max, min, sum, count, pivot
* 也可以自己定义udf函数

## pyspark.sql.Column   & Row

 
 对dataframe的一列做的一些操作, 比如
 
| 函数 |功能 |例子|
| --- | --- | --- |
| alias | 重命名 | df.age.alias('age_new') |
|  between | |df.select(df.name, df.age.between(2, 4)).show() |
| cast 或者astype |将某列转换成另一个dataType| df.select(df.age.cast("string").alias('ages')).collect()|
| contains |  |是否包含的 |

 对dataframe的row做的一些操作
 
 row.asDict(recursive=False)
 
 
## pyspark.sql.Window 
 
 For working with window functions.
 
 -----






## pyspark.sql.functions
 List of built-in functions available for DataFrame.

```python
from pyspark.sql.functions import *
collect_list()
```

注意： collect_list会自动过滤掉数据中的null


## udf 函数

注意： spark的udf函数里传入的是列
example 1.
```python
from pyspark.sql.functions import udf, col
from pyspark.sql.types import StringType

## 定义自己的操作函数
def myfun(s):
    return 'zzz' + s
    
my_udfs = udf(myfun, StringType())          # 将其转为udf，注意需要制定return的类型
# my_udfs = udf(stat_group, MapType(StringType(),ShortType()))   # MapType 指定key，value的格式
df.withColumn("zzz",  my_udfs(df.col1))   # 生成新的一列
```


example2: 在一个myfun中返回多个结果，通过withColunmn生成多列，比如下面的scala代码

```scala
val myUDf = udf((s:String) => Array(s.toUpperCase(),s.toLowerCase()))   #返回两个

val df = sc.parallelize(Seq("Peter","John")).toDF("name")

## 通过取索引，然后drop掉原来结果
val newDf = df
  .withColumn("udfResult",myUDf(col("name")))
  .withColumn("uppercaseColumn", col("udfResult")(0))
  .withColumn("lowercaseColumn", col("udfResult")(1))
  .drop("udfResult")

newDf.show()
```


**example 3: 需要在udf函数中传入其他参数**，例如

```python
def normalize_process(max_value, min_value):
    # 定义一个返回udf的函数
    def process(value):
        if max_value - min_value == 0:
            return 0
        else:
            return (value - min_value) / (max_value - min_value)
        
    return F.udf(process, returnType=FloatType())

df = df.withColumn("area_norm", normalize_process(area_max, area_min)(F.col("area")))
```

```python
## 传入参数词典holiday_dict
def func_is_holiday(holiday_dict):
    return udf(lambda x: holiday_dict.get(x,'-'))
    
df = df.withColumn("is_holiday", func_is_holiday(holiday_dict)(df['floor']) )
```



**example4 将一行转为多行**
https://blog.csdn.net/intersting/article/details/84711723

假设原始的数据是

| Id | data_list |
| --- | --- |
| 111 | [1,2,3] |
| 222 | [4,5,6] |

希望将list中的元素拆出多行，即

| Id | data_new |
| --- | --- |
| 111 | 1 |
| 111 | 2 |
| 111 | 3 |
| 222 | 4 |
| 222 | 5 |
| 222 | 6 |

方法，首先定义一个将array转成词典的udf函数

```python
def udf_array_to_map(array):
    if array is None:
        return array
    return dict((i, v) for i, v in enumerate(array))

idx_udf = udf(lambda x: udf_array_to_map(x), MapType(IntegerType(), IntegerType()))    
```
然后对原始的df进行如下变换：最终就会生成多列，同时还有一个INTEGER_IDX来进行标号

```
df.withColumn('idx_columns', idx_udf(df.data_list)) \
.select('id', f.explode('idx_columns').alias('INTEGER_IDX', 'col'))
```


UDF: 一进一出
UDAF: 聚合函数，多进一出，比如分组统计均值
pandas_udf


另一种方式是使用注册的方式：
spark.udf.register("fun_name",fun_name)


### 常用的格式转化

to_json