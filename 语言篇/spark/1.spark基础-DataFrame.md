# 1.spark基础-DataFrame


spark SQL是spark处理结构化数据的一个模块。


```
df.printSchema()
df.select(df['name'], df['age'] + 1).show()
df.groupBy("age").count().show()
```


**dataframe使用sql的方式进行操作**
SparkSeession的sql函数可以让应用程序以编程的方式运行SQL查询，并将结果以一个dataframe返回。例如：

```
# Register the DataFrame as a SQL temporary view
df.createOrReplaceTempView("people")
sqlDF = spark.sql("SELECT * FROM people")
sqlDF.show()
```



## spark.df 与pandas.df的转化

spark的dataframe与pandas的dataframe略微不同(https://blog.csdn.net/u013613428/article/details/78138857)，可以通过如下的方式进行转化

```python
# spark转pandas
spark_df.toPandas()
# pandas 转spark
sqlContext = SQLContext(SparkContext())
sparkContext= sqlContext.createDataFrame(df)
```
注意如果pandas的dataframe中有空值，在转spark的df时候会报错，需要将缺失数据
`df =df.replace(np.NaN, '')`


pyspark系列--pyspark读写dataframe
https://zhuanlan.zhihu.com/p/34901558

