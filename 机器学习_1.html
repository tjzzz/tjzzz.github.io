
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>
    
  机器学习 - 
  

  </title>
  <meta name="author" content="">
  <meta name="description" content="">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="asset/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="atom.xml" rel="alternate" title="" type="application/atom+xml">
  <script src="asset/js/modernizr-2.0.js"></script>
  <script src="asset/js/jquery.min.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/solarized_light.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>

  <style type="text/css">
  .cat-children-p{ padding: 6px 0px;}
  .hljs{background: none;}
  </style>
  <script type="text/javascript">
  var isAddSildbar = true;
  </script>
  <script src="asset/js/octopress.js" type="text/javascript"></script>
</head>
<script type="text/javascript">
//链接新开窗口
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}
$(document).ready(function(event) {
  addBlankTargetForLinks();
});
</script>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="index.html"></a></h1>
  
    <h2></h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">

  <li id=""><a target="self" href="index.html">Home</a></li>

  <li id=""><a target="_self" href="archives.html">Archives</a></li>

</ul>

</nav>
  <div id="main">
    <div id="content"> 
<div class="blog-index">

	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15384984788761.html">4.5 树模型</a></h1>
			<p class="meta"><time datetime="2018-10-03T00:41:18+08:00" 
			pubdate data-updated="true">2018/10/3</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	

		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15384984375294.html">4.4 k近邻 —— 非参模型</a></h1>
			<p class="meta"><time datetime="2018-10-03T00:40:37+08:00" 
			pubdate data-updated="true">2018/10/3</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>基于实例的学习，其学习的不是明确的泛化模型，而是样本之间的关系。</p>

<p>k近邻方法：先找到与未知样本最接近的k个实例，再根据少数服从多数的准则，给未知样本打上标签。</p>

<ul>
<li>超参数k</li>
<li>距离度量</li>
<li>高维灾难</li>
</ul>

<p>k近邻方法和传统的参数方法不同，参数模型在求得参数后，任务就完成了。新来了一个样本后直接拿训练好的参数进行预测即可；而k近邻不同，它需要将数据存储下来，每来了一个新样本后，要计算离他最近的k个实例。</p>

<p>sklearn中<code>sklearn.neighbors</code>模块</p>

<p>应用</p>

<ul>
<li>无监督的<code>NearestNeighbors</code></li>
</ul>

<p>具体计算的时候可以选择不同的计算方法，&#39;algorithm&#39; =[&#39;auto&#39;, &#39;ball_tree&#39;, </p>

<ul>
<li>有监督的</li>
</ul>

<p>KNeighborsClassifier: 可以指定参数k<br/>
RadiusNeighborsClassifier: 可以指定查询的固定半径r</p>

<pre><code class="language-python">## 官网的示例demo
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn import neighbors, datasets

n_neighbors = 15

# import some data to play with
iris = datasets.load_iris()

# we only take the first two features. We could avoid this ugly
# slicing by using a two-dim dataset
X = iris.data[:, :2]
y = iris.target

h = .02  # step size in the mesh

# Create color maps
cmap_light = ListedColormap([&#39;#FFAAAA&#39;, &#39;#AAFFAA&#39;, &#39;#AAAAFF&#39;])
cmap_bold = ListedColormap([&#39;#FF0000&#39;, &#39;#00FF00&#39;, &#39;#0000FF&#39;])

for weights in [&#39;uniform&#39;, &#39;distance&#39;]:
    # we create an instance of Neighbours Classifier and fit the data.
    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)
    clf.fit(X, y)

    # Plot the decision boundary. For that, we will assign a color to each
    # point in the mesh [x_min, x_max]x[y_min, y_max].
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.figure()
    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

    # Plot also the training points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,
                edgecolor=&#39;k&#39;, s=20)
    plt.xlim(xx.min(), xx.max())
    plt.ylim(yy.min(), yy.max())
    plt.title(&quot;3-Class classification (k = %i, weights = &#39;%s&#39;)&quot;
              % (n_neighbors, weights))

plt.show()
</code></pre>

<p>最近邻回归<br/>
KNeighborsRegressor<br/>
RadiusNeighborsRegressor</p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15384698920685.html">4.3 支持向量机</a></h1>
			<p class="meta"><time datetime="2018-10-02T16:44:52+08:00" 
			pubdate data-updated="true">2018/10/2</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>支持向量机属于上一节中说的硬输出的方法，是从几何角度出发考虑的。</p>

<p>在介绍SVM之前，先解释一下几个基本概念。<br/>
<img src="media/15384698920685/15386228139164.jpg" alt="" style="width:400px;"/></p>

<blockquote>
<p>函数间隔与几何间隔</p>
</blockquote>

<p>一般的，一个点距离分离超平面的远近可以标识分类预测的确信程度。当超平面\(wx+b=0\)确定后，\(|wx+b|\)的大小能够相对的表示远近，而\(wx+b\)的符号与\(y\)类别是否一致则可以表示是否分类正确。 <br/>
\[d_i = y_i(wx_i+b)\]  定义函数间隔\(d = \min d_i\)</p>

<p>但是很明显的会发现这个间隔与\(w\)的量纲是有关系的，\(kw+kb\)与之前超平面是一样的，但是间隔却差了k倍。因此需要<strong>几何间隔</strong></p>

<p>\[d_i = y_i(\frac{w}{||w||}x_i+\frac{b}{||w||}), d = \min d_i\]  </p>

<blockquote>
<p>间隔最大化</p>
</blockquote>

<p>有了前面关于间隔的定义，或者是关于分类距离的定义，就有了目标。SVM的目标就是: 求解能够将数据集正确划分并且几何间隔最大的分离超平面。</p>

<p>注：对于线性可分的数据，超平面有无数个(等价于感知机)，但是间隔最大的只有一个。间隔最大其实是两个要求:</p>

<ul>
<li>能尽可能的将正负例区分开</li>
<li>对于难分的点(距离平面较近的点)也有很好的置信把握，即泛化能力会好一些</li>
</ul>

<blockquote>
<p>SVM对应的三种问题类型</p>
</blockquote>

<p>根据分类问题，可分为以下三种</p>

<ul>
<li>线性可分与硬间隔最大化</li>
<li>线性支持向量机与软间隔最大化</li>
<li>非线性支持向量机与核函数</li>
</ul>

<p>这些会在下面中进行详细介绍</p>

<blockquote>
<p>支持向量</p>
</blockquote>

<p>这个需要看完下面关于三部分的介绍之后，再来了解这个概念。</p>

<ul>
<li>在线性可分的情况下，样本中与超平面距离最近的样本点称为<strong>支持向量</strong>. 支持向量是满足\(y_i(wx_i+b)-1=0\)的点。</li>
<li>分隔边界最终只是由支持向量的点决定的，其他距离比较远的点并没有影响。</li>
</ul>

<h2 id="toc_0">1.线性可分与硬间隔最大化</h2>

<p>先讨论一种最简单的情况，当数据集本身可以线性可分时候(实际数据中可能并没有这么好的性质)，我们只要找到对应的超平面即可。从前面的集合图形上我们的目标：希望每个点到超平面的间隔的最小值达到最大，即\(max min d_i\)</p>

<p>数学模型：\[max_{w,b} d(几何间隔)\]<br/>
\[s.t \quad d_i = y_i(wx_i+b)&gt;= d, i=1,..n\]</p>

<p>因为几何间隔\(d=d&#39;/||w||\)，所以原问题等价于<br/>
\[max_{w,b} \frac{d&#39;}{||w||}\]<br/>
\[s.t \quad d_i = y_i(wx_i+b)&gt;= d&#39;, i=1,..n\]</p>

<p>注意到</p>

<ul>
<li>函数间隔的取值\(d&#39;\)并不影响最优解\(w,b\)的求解。(如果w,b是最优解，则\(kw,kb\)也是，函数距离就是\(d&#39;\)和\(kd&#39;\))。所以这里可以取\(d&#39;=1\)。</li>
<li>\(1/||w||\)与\(\frac{1}{2}||w||^2\)是等价的</li>
</ul>

<p>因此最终优化的问题等价于如下的凸二次规划问题：<br/>
\[min_{w,b} \frac{1}{2}||w||^2\]<br/>
\[s.t \quad d_i = y_i(wx_i+b) -1 &gt;=0, i=1,..n\]</p>

<p>解法：关于这个优化问题的解法，在后面会详细给出。可以证明这个解是存在且唯一的。</p>

<p><strong>几何角度</strong></p>

<ul>
<li>过两个类的边界的点的平行线(\(wx-b=1, wx-b=-1\))，有点类似于楚河汉界</li>
<li>将两条平行线进行旋转，保证同类划分不变。两者距离就会改变</li>
<li>落在两个平行线上的异类点就是<strong>支持向量</strong></li>
</ul>

<p><img src="media/15384698920685/15387096705813.jpg" alt="线性可分示意图"/></p>

<h2 id="toc_1">2.线性支持向量机与软间隔最大化</h2>

<p>上一节说的是对线性可分数据的，如果数据本身不可分该怎么办呢，此时线性分割必然存在误分类的点？对于线性不可分我们不可能要求其严格满足上面说的不等式约束。这时我们只能退而求其次：</p>

<p>思路：使得之前的几何间隔尽量大，同时使误分类的个数尽可能少</p>

<p>数学表示：对于每个样本，类似运筹学中的方法我们可以引入一个松弛变量\(\xi_i\ge 0\),满足\(y_i(wx_i+b)\ge 1-\xi_i\) 之前是要求一定要大于等于1，现在因为并不完全可分，所以不一定要大于等于1。</p>

<p><strong>数学模型：</strong></p>

<p>\[min_{w,b} \frac{1}{2}||w||^2 + C\sum_1^n \xi_i\]<br/>
\[s.t \quad y_i(wx_i+b)&gt;=1-\xi_i, i=1,..n\]<br/>
\[\xi_i &gt;= 0\]</p>

<p>这时候其实可以发现线性可分的相当于是这里的一种特殊情况，即\(\xi_i=0\)</p>

<p><strong>求解：</strong><br/>
该优化问题的拉格朗日函数是<br/>
\[L(w,b,\xi,\alpha,u)=\frac{1}{2}||w||^2+C\sum\xi_i-\sum\alpha_i(y_i(wx_i+b)-1+\xi_i)-\sum u_i\xi_i\]<br/>
因此原问题等价于 \[\max_{\alpha,u}\min_{w,b,\xi}L\]</p>

<p>（1）首先求L对\(w,b,\xi\)的极小<br/>
<img src="media/15384698920685/15387191773648.jpg" alt=""/></p>

<p>可得\[\min_{w,b\xi}L=-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_jy_iy_j(x_i*x_j)+\sum_{i=1}^n\alpha_i\]</p>

<p>(2) 再对\(\min L\)求最大，即有<br/>
\[\max_{\alpha}-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_jy_iy_j(x_i*x_j)+\sum_{i=1}^n\alpha_i\]<br/>
\[s.t.\quad \sum\alpha_iy_i=0\]<br/>
\[C-\alpha_i-u_i&gt;=0;\alpha_i&gt;=0;u_i&gt;=0\]</p>

<p>最后一个条件其实等价于\(0\le\alpha_i\le C\)</p>

<p>对于线性可分问题其max函数是一样的，只是约束条件是\(0\le\alpha_i\)</p>

<p>【定理】若\(\alpha^*=(\alpha_1^*,...\alpha_n)\)是上述优化问题的一个解，若存在一个分量\(0&lt;\alpha_j^*&lt;C\)，则原始问题的解可按如下形式求得 <br/>
\[w^* = \sum\alpha_i^*y_ix_i\]<br/>
\[b^*=y_j-\sum_{i=1}^n y_i\alpha_i^*(x_i*x_j)\]</p>

<p>从中可以发现b的解并不唯一。</p>

<p>最终的分离超平面是\[\sum\alpha_i^*y_i(x_i*x_j)+b^*=0\]<br/>
分类的决策函数是\[f(x)=sign(\alpha_i^*y_i(x_i*x_j)+b^*)\]</p>

<p><strong>几何解释</strong></p>

<ul>
<li>如果在分解边界上，则\(\xi_i=0\),距离是1</li>
<li>如果是在两个分界边界中间，\(0&lt;\xi_i&lt;1\)</li>
<li>越过分界边界，到了另外一边，\(\xi_i &gt;1\) 这个时候\(y_i(wx_i+b)\ge 1-\xi_i\)是小于0的，属于误分类，在优化的时候会考虑</li>
</ul>

<p><img src="media/15384698920685/15387133378663.jpg" alt=""/></p>

<p><strong>合页损失函数</strong>：与原优化问题等价的一种损失函数形式<br/>
\[\min_{w,b} \sum[1-y_i(wx_i+b)]_{+}+\lambda||w||^2\]</p>

<h2 id="toc_2">3.非线性支持向量机与核函数</h2>

<p>当数据的分布呈现的是一种非线性的时候，用之前的线性分割的方法肯定就会出现问题。比如数据是在一个椭圆内外分布，这个时候就需要先对原始数据做一个映射，\(x_1^2,x_2^2\)然后变成线性可分的。而这里主要用到了核技巧。</p>

<p>【核函数】如果存在一个映射\(\phi(x):X-&gt; H\),使得对于所有\(x,z\in X\),函数\(K(x,z)\)满足<br/>
\[K(x,z)=\phi(x)*\phi(z)\]<br/>
则\(K(x,z)\)为核函数，\(\phi(x)\)为映射函数。</p>

<p>核函数的想法是：在学习中值定义核函数，而不显示的定义映射函数，通常映射函数不太好找且不唯一。<br/>
在支持向量机中的应用：<br/>
如前所述，在支持向量机的对偶问题中，只涉及样本之间的内积运算。\[W(\alpha)=\max_{\alpha}-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_jy_iy_j(x_i*x_j)+\sum_{i=1}^n\alpha_i\]<br/>
因此可以用核函数代替<br/>
\[W(\alpha)=\max_{\alpha}-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_jy_iy_jK(x_i,x_j)+\sum_{i=1}^n\alpha_i\]</p>

<p><strong>常用的核函数</strong></p>

<ul>
<li>多项式核函数 \(K(x,z)=(x*z+1)^p\)</li>
<li>高斯核函数 \(K(x,z)=exp(\frac{-||x-z||^z}{2\sigma^2})\)</li>
<li>字符串核函数</li>
</ul>

<h2 id="toc_3">优化算法SMO</h2>

<p>前面介绍了SVM中的三类问题，最终都是通过一个凸二次规划进行求解。当样本量较大的时候，SMO是一种相对求解较快的方法。简言之，是采用了启发式算法，每次只更新两个\(\alpha_i,\alpha_j\)</p>

<p><a href="https://www.cnblogs.com/nolonely/p/6541527.html">https://www.cnblogs.com/nolonely/p/6541527.html</a></p>

<p>Sequential Minimal Optimization A Fast Algorithm for Training Support Vector Machines</p>

<h2 id="toc_4">代码</h2>

<p><code>sklearn.svc</code>模块 <a href="http://scikit-learn.org/stable/modules/svm.html#svm">http://scikit-learn.org/stable/modules/svm.html#svm</a></p>

<p>主要包含</p>

<ul>
<li>SVC 标准的</li>
</ul>

<pre><code>from sklearn import svm
X = [[0, 0], [1, 1]]
y = [0, 1]
clf = svm.SVC()
clf.fit(X, y)  
</code></pre>

<p>以iris数据为例</p>

<pre><code class="language-python">### iris数据
from sklearn import datasets, svm
import matplotlib.pyplot as plt
import numpy as np


iris = datasets.load_iris()
X = iris.data[:, :2]
y = iris.target


# model
C = 1.0
model = svm.SVC(kernel=&#39;linear&#39;, C=C)
model.fit(X, y)



## 绘制可视化图
x1 = X[:,0]
x2 = X[:,1]
h=0.1
x_min, x_max = x1.min()-1, x1.max()+1
y_min, y_max = x2.min()-1, x2.max()+1
xx,yy = np.meshgrid(np.arange(x_min, x_max, h),
                    np.arange(y_min, y_max, h))  #生成对应的坐标矩阵

Z = model.predict(np.c_[xx.ravel(), yy.ravel()])    # 在新的坐标点上预测分类
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)


plt.scatter(x1, x2, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors=&#39;k&#39;)

plt.xlabel(&#39;Sepal length&#39;)
plt.ylabel(&#39;Sepal width&#39;)
</code></pre>

<p><img src="media/15384698920685/15387991752806.jpg" alt=""/></p>

<ul>
<li>NuSVC 与SVC的优化目标函数略微不同</li>
<li>LinearSVC： 只针对linear kernel</li>
</ul>

<p>在进行多分类的时候SVC中参数<code>decision_function_shape</code>可以选择是<code>ovo</code>还是<code>ovr</code><br/>
LinearSVC是通过参数<code>multi_class</code>进行选择，</p>

<h2 id="toc_5">小结</h2>

<p><img src="media/15384698920685/15388009994951.jpg" alt=""/></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15383213879430.html">4.2 回归与分类</a></h1>
			<p class="meta"><time datetime="2018-09-30T23:29:47+08:00" 
			pubdate data-updated="true">2018/9/30</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<blockquote>
<p>统计模型一开始是从回归分析开始的，但是实际问题中更加普遍的是分类问题。那么很自然的一个问题是，如果利用回归分析的方法的话，如何去做分类问题呢？换句话说回归和分类之间的联系在哪里呢？</p>
</blockquote>

<p>一个很自然的想法是，根据回归分析y的结果，按照一定规则将其映射到不同的区间段，从而形成一个分类模型。即连续变量 =&gt; 分类变量</p>

<h3 id="toc_0">1. 硬输出——判别分析</h3>

<p>直接根据阈值划分分类结果， 而这个映射函数即为<strong>判别函数</strong>。比较典型的就是判别分析了。常用的判别分析的方法：</p>

<blockquote>
<p>距离判别法</p>
</blockquote>

<p>即按照样本距离各个总体距离的远近来判断所属的归类。比如基于欧式距离或马氏距离，欧式距离没有考虑到本身特征量纲的差异，所以用的不多。这里以马氏距离，两个类别为例。</p>

<p>例子：已知一个样本X可能来自两个总体\(G_1\)和\(G_2\)其中的一个，其中 \(G_1: \mu_1,V_1, G_2:\mu_2, V_2\), 现在给定X，判断其属于哪一类。<br/>
注意：对总体分布没有要求</p>

<p>解析：判别函数\(W(X)=D^2(X,G_2)-D^2(X,G_1)=(X-\mu_2)V_2^{-1}(X-\mu_2)-(X-\mu_1)V_1^{-1}(X-\mu_1)\).</p>

<p>在实际中因为总体均值和方差是位置的，需要用样本进行估计。当\(V_1=V_2=V\)时候，二次判别式可以变成线性的，有\(W(X) = (X-\bar\mu)V^{-1}(\mu_1-\mu_2)\)， 其中\(\bar\mu = (\mu_1+\mu_2)/2\)</p>

<p>误判概率：判别分析肯定也有出错的时候，下面来讨论下误判概率问题。</p>

<p><img src="media/15383213879430/15384587841040.jpg" alt=""/></p>

<p>从上图中可以比较清楚的看到，出错的概率即为图中阴影部分的面积。</p>

<p>假设X属于1，下面计算其被判断成2的概率即P(2|1)=P(W(X)&lt;0). </p>

<p>令\(\lambda=(\mu_1-\mu_2)&#39;V^{-1}(\mu_1-\mu_2)\). 因为\(W(X) = (X-\bar\mu)V^{-1}(\mu_1-\mu_2)\)， 所以可以计算出<br/>
\(E(W(X))=(\mu_1-\bar\mu)V^{-1}(\mu_1-\mu_2)=\lambda/2\)<br/>
\(D(X)=(\mu_1-\mu_2)&#39;V^{-1}(\mu_1-\mu_2)=\lambda\)<br/>
在X服从正态分布的假定下，有\(W(X)~N(\lambda/2, \lambda)\)，从而可以计算出\(P(2|1)\)与\(P(1|2)\)</p>

<blockquote>
<p>贝叶斯判别法</p>
</blockquote>

<p>前面说的距离判别法因为形式简单，计算简便应用比较广。但是没有考虑到各个总体出现的可能性/概率大小，以及误判之后造成的不同损失大小。而贝叶斯判别法就是考虑这两点的一种判别方法。</p>

<p>这里只对其建模过程做个简述。<br/>
建模：假设m个类别，每个的先验概率和密度函数是 \(q_i, f_i(x)\), 而将原本属于类别i的判给类别j时候造成的损失，记为\(C(j|i)=\)， 而误判的概率可以写成\(p(j|i,R)=\int_{R_j}f_i(x)dx\)。 因此划分规则R最终带来的损失是<br/>
\[g(R)=\sum^m_1 q_i r(i,R)=\sum_1^m q_i\sum_{j=1}^m C(j|i)p(j|i,R)\]</p>

<blockquote>
<p>Fisher判别法</p>
</blockquote>

<p>fisher 判别法出发点和前面两个略有不同，是从投影/降维角度(将多元转为一元)来进行的。以两类为例：假定训练数据分为两个类别C1和C2,每个类别的均值分别是\(u_1\)和\(u_2\),如前所述经过\(y=W^TX+b\)投影后，最终的目标是希望类内的距离尽量小，类间的距离尽量大</p>

<p>类内距离：\(s_k=\sum_{x_i\in C_k}(w^Tx_i-w^Tu_k)^2   k=1,2\)<br/>
类间聚类：\((w^Tu_1 - w^Tu_2)^2\)</p>

<p>因此最终是最大化这个目标函数\(J(W)=\frac{(w^Tu_1 - w^Tu_2)^2}{s_1^2+s_2^2}\)，为了保证\(W\)的唯一性，不妨设\(W&#39;S_WW=1\)</p>

<p>\[MAX J(W)=\frac{W^TS_BW}{W^TS_WW}\]<br/>
\[st. W&#39;S_WW=1 \]</p>

<p>这里简单推导一下求解过程，令<br/>
\[L(W;\lambda)=W&#39;S_BW-\lambda(W&#39;S_WW -1) \]<br/>
\[\frac{\partial J}{\partial w}=2S_BW-2\lambda S_ww=0\]<br/>
\[\frac{\partial J}{\partial \lambda}=W&#39;S_WW - 1=0\]<br/>
=&gt; \(W&#39;S_BW=\lambda\)，所以只需最大化\(\lambda\)<br/>
\(S_BW=\lambda S_ww\)  ==&gt; \(S_W^{-1}S_BW=\lambda W\)<br/>
因此取\(S_W^{-1}S_B\)的最大特征值即为\(\lambda\)，对应的特征向量即为\(W\)</p>

<p>当然除了判别分析之外，还有SVM等方法，这个后续会具体介绍</p>

<h3 id="toc_1">2.软输出</h3>

<p>利用似然度区分回归结果，根据回归值和似然性的关系输出样本属于某个类别的<strong>概率</strong>。即通过一个激活函数，架起了回归和分类问题的通道。<br/>
\[y(x) = g^{-1}(w^Tx+b)\]</p>

<p>逻辑回归就是一个典型的例子。传统的回归模型的假定已经不成立，因为预测变量y是服从二项分布，逻辑回归估计的是样本属于某个类别的后验概率。即</p>

<p>\[p(c_1|x)=\frac{p(x|c_1)p(c_1)}{p(x|c_1)p(c_1) + p(x|c_2)p(c_2)}\]</p>

<p>从这个角度看，对上式稍加变形就有</p>

<p>\[p(c_1|x)=\frac{1}{1+exp(-z)}=\pi(z)\]<br/>
其中\(z=ln \frac{p(x|c_1)p(c_1)}{p(x|c_2)p(c_2)}=w^Tx + b\)</p>

<p>参数估计用MLE方法：最终分类结果只有两个，所以是服从二项分布<br/>
\[p(w,b|x) = \prod \pi(x_i)^{y_i}[1-\pi(x_i)]^{1-y_i}\]</p>

<p>对数似然函数<br/>
\[J(w)=\sum_1^n y_i ln(\pi(z_i)) + (1-y_i)ln(1-\pi(z_i))\]</p>

<p>最大化这个目标函数，可以用梯度下降的方法。注意到sigmoid函数有一个比较好的性质\(\pi&#39;(z)=\pi(z)(1-\pi(z))\)</p>

<p>推导：</p>

<p>\[\frac{\partial J(W)}{\partial w_j}=\sum (y_i\frac{1}{pi(z_i)}-(1-y_i)\frac{1}{1-\pi(z_i)})\pi(z_i)(1-\pi(z_i))x_j\]<br/>
\[=\sum [y_i(1-\pi(z_i)) - (1-y_i)\pi(z_i)]x_j\]</p>

<p>\[=\sum(y_i -\pi(z_i))x_j\]</p>

<p>因此在利用梯度下降方法更新权重时候，只需要按照\(w_j:=w_j +\eta\sum(y_i -\pi(z_i))x_j\)即可</p>

<p>梯度下降的python版本实现 <a href="https://github.com/tjzzz/data_science/blob/master/statistical%20learning/logistic_regression.py">github</a></p>

<p><strong>sklearn实现</strong>： 逻辑回归是在<code>sklearn.linear_model</code></p>

<pre><code>from sklearn.linear_model import LogisticRegression
model = LogisticRegression(multi_class=&#39;multinomial&#39;, solver=&#39;sag&#39;)
model.fit(X_train,y_train)
y_pred = model.predict(X_test)
</code></pre>

<blockquote>
<p>OR值(odds ratio), 本来是在医学实验中的词，值实验组中暴露人数与非暴露人数的比值，与对照组中暴露人数与非暴露人数的比值的比值，所以也叫<strong>比值比</strong><br/>
逻辑回归的更新公式与一般的线性回归的极为相似。\(\sum(y_i-x_iw)x_j\)</p>
</blockquote>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15382821285011.html">3.1 特征处理—— 特征选择</a></h1>
			<p class="meta"><time datetime="2018-09-30T12:35:28+08:00" 
			pubdate data-updated="true">2018/9/30</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<h2 id="toc_0">参考</h2>

<p><a href="https://blog.csdn.net/kebu12345678/article/details/78437118">https://blog.csdn.net/kebu12345678/article/details/78437118</a></p>


		</div>

		

	</article>
  
	<div class="pagination">
	 <a class="prev" href="机器学习_2.html">&larr; Older</a> 
<a href="archives.html">Blog Archives</a>
	 <a class="next" href="机器学习.html">Newer &rarr;</a>  
	    
	</div>
</div>
 <aside class="sidebar"> 

	<section>
	  <h1>Categories</h1>
	  <ul id="recent_posts">
	  
	      <li class="post">
	        <a href="%E4%BD%A0%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%9F%A5%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95.html"><strong>你不得不知的统计方法&nbsp;(3)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%BA%B3%E7%B1%B3%E5%AD%A6%E4%BD%8D.html"><strong>Udacity.DL&nbsp;(3)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="DL.html"><strong>Ng.DL&nbsp;(3)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="book.DL.html"><strong>book.DL&nbsp;(8)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>机器学习&nbsp;(23)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.html"><strong>推荐系统&nbsp;(3)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="NLP&%E5%9B%BE%E5%83%8F.html"><strong>NLP&图像&nbsp;(4)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>数据可视化&nbsp;(7)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="0.%20%E8%B5%84%E6%BA%90%E5%AF%BC%E8%88%AA.html"><strong>0. 资源导航&nbsp;(3)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="data.html"><strong>data&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	   
	  </ul>
	</section>
	<section>
	  <h1>Recent Posts</h1>
	  <ul id="recent_posts">
	  
	      
		      <li class="post">
		        <a href="15516671779096.html">list</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15479712192030.html">R 数据透视表</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15456330400571.html">统计阈值</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15394437873112.html">4.9 深度学习综述</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15394428762655.html">4.8 自适应的基函数——神经网络</a>
		      </li>
	     
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	   
	  </ul>
	</section>
	
</aside> </div></div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  -
  <span class="credit">Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; Theme by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>

  
    


<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

</body>
</html>