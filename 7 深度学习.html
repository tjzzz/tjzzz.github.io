
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>
    
  7 深度学习 - 学习笔记
  

  </title>
  <meta name="author" content="">
  <meta name="description" content="">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="asset/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="atom.xml" rel="alternate" title="学习笔记" type="application/atom+xml">
  <script src="asset/js/modernizr-2.0.js"></script>
  <script src="asset/js/jquery.min.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/solarized_light.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>

  <style type="text/css">
  .cat-children-p{ padding: 6px 0px;}
  .hljs{background: none;}
  </style>
  <script type="text/javascript">
  var isAddSildbar = true;
  </script>
  <script src="asset/js/octopress.js" type="text/javascript"></script>
</head>
<script type="text/javascript">
//链接新开窗口
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}
$(document).ready(function(event) {
  addBlankTargetForLinks();
});
</script>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="index.html">学习笔记</a></h1>
  
    <h2></h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">

  <li id=""><a target="self" href="index.html">Home</a></li>

  <li id=""><a target="_self" href="archives.html">Archives</a></li>

</ul>

</nav>
  <div id="main">
    <div id="content"> 
<div class="blog-index">

	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15596648844646.html">【Tensorflow】week2 Introduction to Computer Vision</a></h1>
			<p class="meta"><time datetime="2019-06-05T00:14:44+08:00" 
			pubdate data-updated="true">2019/6/5</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>前面说的hello word的例子，输入输出比较简单，这里考虑稍微复杂一点的情况—— 输入数据是图像</p>

<p>人类看到下面的图像之后，能很清楚的分辨出哪个是裤子，哪个是鞋子，那么计算机要怎么分辨呢？</p>

<p><img src="media/15596648844646/15600952203749.jpg" alt=""/></p>

<h2 id="toc_0">1 fashion-mnist数据集介绍</h2>

<p>图像数据集 <br/>
<a href="https://github.com/zalandoresearch/fashion-mnist">https://github.com/zalandoresearch/fashion-mnist</a></p>

<p>这个数据集是MNIST dataset——手写识别数字的的数据集的一个替换集合。数据格式上和手写识别数字的数据一样。</p>

<pre><code>import tensorflow as tf
import numpy as np
from tensorflow import keras

model = tf.keras.Sequential([
keras.layers.Dense(units=1, input_shape=[1])])



model.compile(optimizer=&#39;sgd&#39;, loss=&#39;mean_squared_error&#39;)
xs = np.array([1,2,3,4,5,6,7], dtype=float)
ys = np.array([100, 150, 200,250, 300,350, 400], dtype=float)
model.fit(xs, ys, epochs=500)
print(model.predict([7.0]))
</code></pre>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15596470753558.html">TensorFlow</a></h1>
			<p class="meta"><time datetime="2019-06-04T19:17:55+08:00" 
			pubdate data-updated="true">2019/6/4</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>入门教程</p>

<p><a href="https://github.com/machinelearningmindset/TensorFlow-Course">https://github.com/machinelearningmindset/TensorFlow-Course</a></p>

<h2 id="toc_0">1.week1</h2>

<p>传统编程与深度学习编程的区别</p>

<p><img src="media/15596470753558/15596480786004.jpg" alt=""/></p>

<p>安装tensorflow<br/>
<a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/1-2-install/">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/1-2-install/</a></p>

<pre><code># 安装CPU/GPU版本。需pip至少8.1版本
pip install tensorflow
pip install tensorflow-gpu
</code></pre>

<p>中间有报警<br/>
<img src="media/15295097623789/15295102306245.jpg" alt=""/></p>

<p>最后, 如果你需要升级 Tensorflow 的版本, 推荐的方式是：根据你的 python 版本, 在 terminal 中删除原有的版本,然后再装一遍</p>

<pre><code># 如果你是 Python 2, 请复制下面
pip uninstall tensorflow
# 如果你是 Python 3, 请复制下面
pip3 uninstall tensorflow
</code></pre>

<p>安装成功后</p>

<pre><code>&gt; import tensorflow as tf
</code></pre>

<p>报错：<img src="media/15295097623789/15295150578030.jpg" alt=""/></p>

<p>解决；<a href="http://www.laomn.com/article/item/60445">http://www.laomn.com/article/item/60445</a><br/>
<a href="https://www.cnblogs.com/atomicbomb/p/7028917.html">https://www.cnblogs.com/atomicbomb/p/7028917.html</a><br/>
<a href="https://blog.csdn.net/ChenVast/article/details/79103288">https://blog.csdn.net/ChenVast/article/details/79103288</a></p>

<p>mac上安装正常，没有问题</p>

<h2 id="toc_1">week1 hello word</h2>

<p>Original file is located at<br/>
    <a href="https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Exercises/Exercise%201%20-%20House%20Prices/Exercise_1_House_Prices_Question.ipynb">https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/Exercises/Exercise%201%20-%20House%20Prices/Exercise_1_House_Prices_Question.ipynb</a></p>

<p>In this exercise you&#39;ll try to build a neural network that predicts the price of a house according to a simple formula.</p>

<p>So, imagine if house pricing was as easy as a house costs 50k + 50k per bedroom, so that a 1 bedroom house costs 100k, a 2 bedroom house costs 150k etc.</p>

<p>How would you create a neural network that learns this relationship so that it would predict a 7 bedroom house as costing close to 400k etc.</p>

<p>Hint: Your network might work better if you scale the house price down. You don&#39;t have to give the answer 400...it might be better to create something that predicts the number 4, and then your answer is in the &#39;hundreds of thousands&#39; etc.</p>

<p>这里先以一个简单的”hello word“例子来介绍，假设我们想学习一个简单的线性关系<br/>
\[y=50x+50 \]</p>

<p>我们已知是输入数据x，以及答案Y，想知道的其实是背后x和y之间的关系(rule)。那么机器要怎么学习这种关系呢？</p>

<pre><code># 线性回归例子
import tensorflow as tf
import numpy as np
from tensorflow import keras

model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])
model.compile(optimizer=&#39;sgd&#39;, loss=&#39;mean_squared_error&#39;)
xs = np.array([1,2,3,4,5,6,7], dtype=float)
ys = np.array([100, 150, 200,250, 300,350, 400], dtype=float)
model.fit(xs, ys, epochs=500)
print(model.predict([7.0]))
</code></pre>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15209459008411.html">第八章 深度模型中的优化</a></h1>
			<p class="meta"><time datetime="2018-03-13T20:58:20+08:00" 
			pubdate data-updated="true">2018/3/13</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<h2 id="toc_0">8.1 优化与学习</h2>

<p>本章主要关注这一类特定的优化问题:寻找神经网络上的一组参数 θ，它能显 著地降低代价函数 J(θ)，该代价函数通常包括整个训练集上的性能评估和额外的正 则化项。</p>

<p>通常来说极其学习的算法目标大概可以写成，降低期望泛化误差。即<strong>风险</strong><br/>
<img src="media/15209459008411/15209462294325.jpg" alt=""/></p>

<p>其中p_data是真实的分布，实际中我们只能最小化<strong>经验风险</strong><br/>
<img src="media/15209459008411/15209463229280.jpg" alt=""/></p>

<h2 id="toc_1">8.2神经网络优化中的挑战</h2>

<ul>
<li>病态： hessain矩阵病态</li>
<li>局部最小值：对于非凸函数可能存在多个局部最小值。</li>
<li>。。。</li>
</ul>

<h2 id="toc_2">8.3基本算法</h2>

<p>梯度下降机器变种一般是机器学习中应用较多的优化算法。</p>

<h3 id="toc_3">(1)随机梯度下降SGD</h3>

<p><img src="media/15209459008411/15221677256921.jpg" alt=""/></p>

<h3 id="toc_4">（2）动量</h3>

<p>动量是比SGD速度更快的一种方法，SGD相当于只是考虑梯度的方向，而动量是在此基础上加上速度。</p>

<p><img src="media/15209459008411/15221680917826.jpg" alt=""/></p>

<h2 id="toc_5">8.4 参数初始化</h2>

<h2 id="toc_6">参考资料</h2>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15120469131645.html">第七章 正则化</a></h1>
			<p class="meta"><time datetime="2017-11-30T21:01:53+08:00" 
			pubdate data-updated="true">2017/11/30</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>机器学习不止要求在训练数据上有良好的表现，还希望有较好的泛化能力，即在测试数据上也能减少测试误差。这些策略被统称为<strong>正则化</strong></p>

<p>常见的一些正则化方法：</p>

<h2 id="toc_0">1.参数范数惩罚</h2>

<p>即在原有的优化函数中，添加一个对参数的惩罚，来限制模型的学习能力<br/>
<img src="media/15120469131645/15206725126887.jpg" alt=""/></p>

<p>(1) L2参数正则化<br/>
L2正则化/岭回归，又叫权重衰减； 可以参照岭回归<br/>
<img src="media/15120469131645/15206732558707.jpg" alt=""/></p>

<p>(X⊤X + αI)−1 这个新矩阵与原来的是一样的，不同的仅仅是在对 角加了 α。这个矩阵的对角项对应每个输入特征的方差。我们可以看到，L2正则化能 让学习算法 ‘‘感知’’ 到具有较高方差的输入 x，因此与输出目标的协方差较小(相对 增加方差)的特征的权重将会收缩。</p>

<p>（2）L1正则化： 具有稀疏性，很多会衰减成0，所以有时候会用来做变量选择lasso</p>

<h2 id="toc_1">2.作为约束的范数惩罚</h2>

<p>从另一个角度来看这个问题，可以被看做是构造一个广义拉格朗日函数来最小化带约束的函数</p>

<p><img src="media/15120469131645/15206742273357.jpg" alt=""/></p>

<p><img src="media/15120469131645/15206742365336.jpg" alt=""/></p>

<p>最小化约束 + 重投影的角度</p>

<h2 id="toc_2">3.正则化和欠约束问题</h2>

<p>正则化还有个好处可以保证X&#39;X+aI 是可逆的</p>

<h2 id="toc_3">4.数据集增强</h2>

<p>可以通过一些方法在不改变label的情况下自己构造一批数据集，比如图像识别时候对图像的旋转；有些时候也可以通过输入噪声的方式来进行数据集增强。</p>

<h2 id="toc_4">5.噪声鲁棒性</h2>

<ul>
<li>向模型的输入或者是权重中加入噪音。</li>
<li>向输出目标中加入噪音，即标注数据有一定的错误，不是百分百准确的时候。<strong>标签平滑</strong>方法是将0，1分类，变为e/(k-1)和1-e的k个输出的softmax函数。（e是标注的错误率）</li>
</ul>

<h2 id="toc_5">6.半监督学习</h2>

<p>大概意思是说：P(x) 产生的未标记样本和P(x,y)中的标记样本都用 于估计 P(y|x)或者根据x预测y。</p>

<h2 id="toc_6">7.多任务学习</h2>

<p>就是通过合并几个任务中的样例（可以视为对参数施加的软约束），我感觉就是类似于group_lasso这样的，对参数加了一个别的约束，部分样本需要共享同一个参数</p>

<h2 id="toc_7">8.提前终止</h2>

<p><img src="media/15120469131645/15206764319925.jpg" alt=""/><br/>
当训练的能力较强时候会发现，随着训练次数的增加，训练误差在逐步减小，但是测试误差会呈现一个U型状态。即先减小后增加。</p>

<p>不是从模型的优化函数入手，相当于是一个实践的经验技巧。</p>

<p>提前终止具有正则化的效果</p>

<h2 id="toc_8">9.参数绑定和参数共享</h2>

<h2 id="toc_9">10.稀疏表示</h2>

<p>前文所述的权重衰减直接惩罚模型参数。另一种策略是惩罚神经网络中的激活 单元，稀疏化激活单元。这种策略间接地对模型参数施加了复杂惩罚。<br/>
<img src="media/15120469131645/15207643423743.jpg" alt=""/></p>

<h2 id="toc_10">11.Bagging 和其他集成方法</h2>

<p>结合多个模型，进行模型平均</p>

<h2 id="toc_11">12.dropout</h2>

<p>@@@@</p>

<h2 id="toc_12">13.对抗训练</h2>

<h1 id="toc_13">14 切面距离、正切传播和流形正切分类器</h1>

<p>略</p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15090214789463.html">第六章 深度前馈网络</a></h1>
			<p class="meta"><time datetime="2017-10-26T20:37:58+08:00" 
			pubdate data-updated="true">2017/10/26</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>前馈神经网络，也叫做<strong>多层感知机</strong>(MLP),其目标是近似某个函数\(f^*\)。前馈网络定义了一个映射函数\(y=f(x,\theta)\),然后学习参数\(\theta\)的值，从而近似函数\(f^*\)</p>

<p><strong>前馈</strong></p>

<p>因为在模型的输出和模型本身之间没有反馈链接。一些基本术语概念：</p>

<ul>
<li>输出层，隐藏层，</li>
</ul>

<h2 id="toc_0">6.1 例子：学习XOR</h2>

<p>【例】学习异或逻辑，即当两个二进制值中恰好有一个是1时候返回1，其他都返回0.</p>

<p>【解法1】可以考虑按照回归分析的思路，使用均方误差作为损失函数(实际中对于二进制数据建模时不适合用MSE)。\(f(x,w,b)=x^Tw+b\)<br/>
可以利用矩阵求解方程得到w=0,b=0.5， 线性模型得到的结果是任意一点都为0.5<br/>
==&gt; 主要问题，该问题其实不是一个线性问题</p>

<p>【解】另一种思路：<strong>使用一个模型来学习一个不同的特征空间，在这个空间上线性模型能够表示这个问题。</strong><br/>
<img src="media/15090214789463/15093791249803.jpg" alt=""/></p>

<p>构造一个非线性函数\(h\)，一般默认的推荐是<strong>整流线性单元</strong>ReLU:<br/>
\[g(z)=max(0, z)\]<br/>
所以整个网络是\(f(x)=w^Tmax(0, w^Tx+c)+b\)</p>

<h2 id="toc_1">6.2 基于梯度的学习</h2>

<p>很多神经网络的非线性导致代价函数变得非凸，所以一般神经网络的训练一般使用迭代的方法。<br/>
凸优化从任何一种初始参数都会收敛，而用于非凸损失函数的随机梯度下降没有这种收敛的保障，并且对参数的初始值比较敏感。</p>

<h3 id="toc_2">代价函数</h3>

<p>多数情况下，对于一个参数模型\(f(y|x,\theta)\)，</p>

<ul>
<li>可以简单的使用最大似然原理。即利用训练数据和模型预测之间的<strong>交叉熵</strong>作为代价函数。</li>
<li>有时候也会出现仅预测在给定x条件下y的某些统计量。</li>
</ul>

<p><strong>最大似然学习条件分布</strong></p>

<p>大多数的神经网络采用最大似然的训练方法。这意味着代价函数就是负的对数似然。它与训练数据</p>

<p>\[J(\theta) = -E log p_{model}(y|x)\]</p>

<p><strong>学习条件统计量</strong></p>

<p>这里好像有个和高统中类似的结论。比如<br/>
\[f^*=arg min_{f} E_{x,y~p}||y-f(x)||^2\]<br/>
可以得到\(f^*=E_{y~p(y|x)}(y)\)</p>

<h3 id="toc_3">输出单元</h3>

<p>其实代价函数的选择与输出单元是有一定关系的</p>

<ul>
<li>高斯输出分布的线性单元</li>
</ul>

<p>线性输出层一般用来产生高斯分布的均值。</p>

<ul>
<li>伯努利输出分布的sigmoid单元</li>
<li>多项分布输出的softmax单元</li>
</ul>

<p>对于想要表示具有n个离散取值的变量分布时候，一般可以用softmax函数。其是sigmoid函数在多分类问题上的推广。<br/>
\[softmax(z_j)=\frac{exp(z_i)}{\sum_j exp(z_j)}\]</p>

<h2 id="toc_4">6.3 隐藏单元</h2>

<p>隐藏层单元的选择没有一个固定的指导标准，一般默认选择<strong>整流线性单元</strong></p>

<p><strong>整流线性单元的一些扩展</strong></p>

<p>其扩展形式主要是基于\(z_i&lt;0\)时候使用一个非零的斜率\(\alpha_i\)</p>

<p>绝对值整流、渗漏整流线性单元，参数化整流线性单元。</p>

<p><strong>maxout单元</strong><br/>
maxout将单元将z划分成k个组，每个组上的取值为其最大值。</p>

<p><strong>sigmoid，tanh</strong></p>

<h2 id="toc_5">6.4网络结构设计</h2>

<p>网络结构设计主要是设计网络的<strong>深度</strong>和每一层的<strong>宽度</strong></p>

<p><strong>1. 万能近似性质</strong></p>

<p>万能近似定力：一个前馈神经网络,如果具有线性输出层和至少一个具有任何一种“挤压”性质的寄货函数的隐藏层。只要给予网络足够数量的隐藏单元，它可以以任意精度来近似任何一个有限维空间到另一个有限维空间的Borel可测函数。</p>

<p>万能定理从理论上说明无论我们想要学习什么函数，都有一个大的MLP可以进行这个。但是我们并不能保证算法能够学习到这个函数</p>


		</div>

		

	</article>
  
	<div class="pagination">
	 <a class="prev" href="7 深度学习_1.html">&larr; Older</a> 
<a href="archives.html">Blog Archives</a>
	 
	    
	</div>
</div>
 <aside class="sidebar"> 

	<section>
	  <h1>Categories</h1>
	  <ul id="recent_posts">
	  
	      <li class="post">
	        <a href="%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6-%E6%B8%85%E5%8D%95.html"><strong>数据科学-清单&nbsp;(4)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="1%20Tools.html"><strong>1 Tools&nbsp;(33)</strong></a>
	         <p class="cat-children-p"> 
	        
	        	<a href="%E9%85%B7%E7%82%AB%E7%A5%9E%E5%99%A8.html">酷炫神器&nbsp;(12)</a>&nbsp;&nbsp;
	        
	        	<a href="PaddlePaddle.html">PaddlePaddle&nbsp;(3)</a>&nbsp;&nbsp;
	        
	        	<a href="spark.html">spark&nbsp;(12)</a>&nbsp;&nbsp;
	        
	        	<a href="tensorflow.html">tensorflow&nbsp;(2)</a>&nbsp;&nbsp;
	        
	        	<a href="SQL.html">SQL&nbsp;(1)</a>&nbsp;&nbsp;
	        
	         </p> 
	      </li>
	  
	      <li class="post">
	        <a href="2%20Get%20Data.html"><strong>2 Get Data&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="3%20%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>3 数据可视化&nbsp;(8)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="4%20%E4%BD%A0%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%9F%A5%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95.html"><strong>4 你不得不知的统计方法&nbsp;(5)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="5%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>5 机器学习&nbsp;(23)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.html"><strong>6 推荐系统&nbsp;(3)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="6%20NLP&%E5%9B%BE%E5%83%8F.html"><strong>6 NLP&图像&nbsp;(4)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="7%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html"><strong>7 深度学习&nbsp;(11)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="8%20Ai%E5%BA%94%E7%94%A8.html"><strong>8 Ai应用&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E8%AE%B8%E5%BC%8F%E4%BC%9F-%E6%9E%B6%E6%9E%84%E8%AF%BE.html"><strong>许式伟-架构课&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	   
	  </ul>
	</section>
	<section>
	  <h1>Recent Posts</h1>
	  <ul id="recent_posts">
	  
	      
		      <li class="post">
		        <a href="15623124260572.html">1 抽样方法</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15598839206919.html">SC-FEGAN</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15596648844646.html">【Tensorflow】week2 Introduction to Computer Vision</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15596470753558.html">TensorFlow</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15594599843014.html">估计的置信度</a>
		      </li>
	     
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	   
	  </ul>
	</section>
	
</aside> </div></div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  -
  <span class="credit">Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; Theme by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>

  
    


<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

</body>
</html>