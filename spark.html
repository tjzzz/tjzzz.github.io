
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>
    
  spark - 学习笔记
  

  </title>
  <meta name="author" content="">
  <meta name="description" content="">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="asset/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="atom.xml" rel="alternate" title="学习笔记" type="application/atom+xml">
  <script src="asset/js/modernizr-2.0.js"></script>
  <script src="asset/js/jquery.min.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/solarized_light.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>

  <style type="text/css">
  .cat-children-p{ padding: 6px 0px;}
  .hljs{background: none;}
  </style>
  <script type="text/javascript">
  var isAddSildbar = true;
  </script>
  <script src="asset/js/octopress.js" type="text/javascript"></script>
</head>
<script type="text/javascript">
//链接新开窗口
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}
$(document).ready(function(event) {
  addBlankTargetForLinks();
});
</script>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="index.html">学习笔记</a></h1>
  
    <h2></h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">

  <li id=""><a target="self" href="index.html">Home</a></li>

  <li id=""><a target="_self" href="archives.html">Archives</a></li>

</ul>

</nav>
  <div id="main">
    <div id="content"> 
<div class="blog-index">

	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15513668402051.html">pyspark提交任务相关配置</a></h1>
			<p class="meta"><time datetime="2019-02-28T23:14:00+08:00" 
			pubdate data-updated="true">2019/2/28</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>上传包<br/>
<a href="https://www.jianshu.com/p/92be93cfbb97">https://www.jianshu.com/p/92be93cfbb97</a></p>

<p><a href="https://blog.csdn.net/weixin_42649077/article/details/84976960">https://blog.csdn.net/weixin_42649077/article/details/84976960</a></p>

<p><a href="http://www.it1352.com/220302.html">http://www.it1352.com/220302.html</a></p>

<p><a href="http://ju.outofmemory.cn/entry/171843">http://ju.outofmemory.cn/entry/171843</a></p>

<h2 id="toc_0">模块上传的可以参照exsense的plugin， 目录在src/experiment</h2>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15442821903932.html">1 安装-mac</a></h1>
			<p class="meta"><time datetime="2018-12-08T23:16:30+08:00" 
			pubdate data-updated="true">2018/12/8</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p><strong>安装准备</strong></p>

<p>(1) 安装jdk8</p>

<p>官网下载， 这个好像比较慢<a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html">https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a></p>

<p>我把下载好的放到了百度网盘上: <a href="https://pan.baidu.com/s/1fpiSse78YObfalSerql4Zw">https://pan.baidu.com/s/1fpiSse78YObfalSerql4Zw</a> 提取码: t6q3</p>

<pre><code>vim ~/.bash_profile
## 添加如下环境变量信息
export JAVA_HOME=&quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_191.jdk/Contents/Home/&quot;
export PATH=&quot;$JAVA_HOME/bin:$PATH&quot;
</code></pre>

<p>(2) 官网下载 <a href="http://mirror.bit.edu.cn/apache/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz">http://mirror.bit.edu.cn/apache/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz</a><br/>
或者 <code>brew install apache-spark</code></p>

<p>然后将文件解压到想要放到的目录，我这里是/Users/zhengzhenzhen/ 为例方便版本管理以及后续使用，可以创建一个软链</p>

<pre><code>ln -s /Users/zhengzhenzhen/spark-2.4.0-bin-hadoop2.7 /Users/zhengzhenzhen/spark
</code></pre>

<p>（3） </p>

<pre><code>pip install pyspark
</code></pre>

<p><strong>使用jupyter nootbook</strong></p>

<p>在jupyter notebook中使用pyspark，需要配置环境变量</p>

<pre><code># 基于python的版本
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS=&#39;notebook&#39;
</code></pre>

<h2 id="toc_0">参考资料</h2>

<p><a href="https://www.jianshu.com/p/f284b77289ca">https://www.jianshu.com/p/f284b77289ca</a></p>

<p><a href="https://www.jianshu.com/p/cc83d047383d">https://www.jianshu.com/p/cc83d047383d</a></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15417735380783.html">任务迁移</a></h1>
			<p class="meta"><time datetime="2018-11-09T22:25:38+08:00" 
			pubdate data-updated="true">2018/11/9</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p><a href="http://wiki.baidu.com/pages/viewpage.action?pageId=496903743">http://wiki.baidu.com/pages/viewpage.action?pageId=496903743</a></p>

<p><a href="http://wiki.baidu.com/display/SPRK/Tutorial+for+Spark-baidu">http://wiki.baidu.com/display/SPRK/Tutorial+for+Spark-baidu</a></p>

<h2 id="toc_0">庆城</h2>

<p>17年的spark资源已经到位，kce组2台机器，1095核，216549M内存，存储51.2T，队列为spark-ubs-kce, 客户端拉取地址：<br/>
wget <a href="ftp://cp01-ubskce25.epc.baidu.com/home/work/spark-client-tianqi-2.1.tar.gz">ftp://cp01-ubskce25.epc.baidu.com/home/work/spark-client-tianqi-2.1.tar.gz</a><br/><br/>
百度spark wiki地址：<a href="http://wiki.baidu.com/display/SPRK/Baidu+Spark">http://wiki.baidu.com/display/SPRK/Baidu+Spark</a><br/>
提交任务命令示例：./spark-client-tianqi-2.1/bin/spark-submit --master yarn --deploy-mode client --queue spark-ubs-kce ./test.py<br/>
任务进度查看页面：<a href="http://yq01-tianqi-normandy.dmop.baidu.com:8033/queue.html?queue=spark-ubs-kce&amp;physical=tianqi-yarn_phy">http://yq01-tianqi-normandy.dmop.baidu.com:8033/queue.html?queue=spark-ubs-kce&amp;physical=tianqi-yarn_phy</a><br/>
欢迎各位试用，谢谢！</p>

<h2 id="toc_1">1.pyspark 读取udw表</h2>

<pre><code class="language-{python}">spark = SparkSession.builder.appName(&quot;video_test_&quot; + day).getOrCreate()
video_session_sql = &quot;select event_time, event_cookie, event_urlparams, event_query,  event_others, wisepsdisplay_qid, wisepsdisplay_pn,  wisepsdisplay_tn, &quot; + \
        &quot; wisepsdisplay_displayresult, wisepsdisplay_sampletags, clk_detail, media_time, event_browser, event_os, &quot; + \
        &quot; event_os_version, event_pd, event_cuid, anti_spam, event_day &quot; + \
        &quot; from ud_ml_sf_merge_data where event_day= &quot; + day + &quot; limit 100&quot;

video_session = spark.sql(video_session_sql)
</code></pre>

<p>video_session是一个dataframe，可以直接取出里面的相关字段做特征提取</p>

<h2 id="toc_2">2.特征提取</h2>

<p>通过map函数对每条记录进行特征提取</p>

<pre><code>def parser_record(row):
    # do something
    
    # 返回key，value形式
    return (key, value)
    
    
video_session.rdd.map(parser_record)
</code></pre>

<h2 id="toc_3">合并汇总</h2>

<p>假设map输出格式是 (key, [1,2,3]), 下面对相同的key的list中的指标值进行累加</p>

<pre><code>video_session.rdd.map(simple_test).reduceByKey(lambda x,y : x + y)   # 单个值的累加
video_session.rdd.map(simple_test).reduceByKey(lambda x,y : [x[i] + y[i] for i in range(len(x))])
</code></pre>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15124636215412.html">1.spark基础-DataFrame</a></h1>
			<p class="meta"><time datetime="2017-12-05T16:47:01+08:00" 
			pubdate data-updated="true">2017/12/5</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>spark SQL是spark处理结构化数据的一个模块。</p>

<pre><code>df.printSchema()
df.select(df[&#39;name&#39;], df[&#39;age&#39;] + 1).show()
df.groupBy(&quot;age&quot;).count().show()
</code></pre>

<p>SparkSeession的sql函数可以让应用程序以编程的方式运行SQL查询，并将结果以一个dataframe返回。例如：</p>

<pre><code># Register the DataFrame as a SQL temporary view
df.createOrReplaceTempView(&quot;people&quot;)
sqlDF = spark.sql(&quot;SELECT * FROM people&quot;)
sqlDF.show()
</code></pre>

<p>pyspark系列--pyspark读写dataframe<br/>
<a href="https://zhuanlan.zhihu.com/p/34901558">https://zhuanlan.zhihu.com/p/34901558</a></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15124622385174.html">建立连接 sparkSession</a></h1>
			<p class="meta"><time datetime="2017-12-05T16:23:58+08:00" 
			pubdate data-updated="true">2017/12/5</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>spark的入口方式。</p>

<p>spark建立连接的方式一般有如下两种</p>

<pre><code>from pyspark.sql import SparkSession

spark = SparkSession.builder.appName(&quot;CountVectorizerExample&quot;).getOrCreate()
sc = spark.sparkContext

</code></pre>

<pre><code>from pyspark import SparkContext
sc = SparkContext(appName=&quot;Word2VecExample&quot;)
</code></pre>

<p>在Spark的早期版本，sparkContext是进入Spark的切入点。我们都知道RDD是Spark中重要的API，然而它的创建和操作得使用sparkContext提供的API；对于RDD之外的其他东西，我们需要使用其他的Context。比如对于流处理来说，我们得使用StreamingContext；对于SQL得使用sqlContext；而对于hive得使用HiveContext。然而DataSet和Dataframe提供的API逐渐称为新的标准API，我们需要一个切入点来构建它们，所以在 Spark 2.0中我们引入了一个新的切入点(entry point)：SparkSession</p>

<p>SparkSession实质上是SQLContext和HiveContext的组合（未来可能还会加上StreamingContext），所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的<br/>
　　<br/>
　　</p>

<h1 id="toc_0">2.矩阵相关</h1>

<p><a href="http://rdc.hundsun.com/portal/article/691.html">http://rdc.hundsun.com/portal/article/691.html</a></p>

<h2 id="toc_1">缓存</h2>

<p>能够把数据缓存在集群的内存里。这通过调用RDD的cache函数来实现：<code>rddFromTextFile.cache()</code></p>

<h2 id="toc_2">sc.textFile()</h2>

<h2 id="toc_3">sparksession</h2>

<p>将文本数据读入data.frame<br/>
examples/src/main/python/ml/dataframe_example.py:    newDF = spark.read.parquet(tempdir)</p>

<p>dataframe</p>

<pre><code>df.printSchema()    #打印出列的名称和类型
df.show()   # 查看数据
* show(num，是否截断)
df..select(&quot;studentName&quot;, &quot;email&quot;)
</code></pre>

<p>当一个字典不能被提前定义 (例如,记录的结构是在一个字符串中, 抑或一个文本中解析, 被不同的用户所属), 一个 DataFrame 可以通过以下3步来创建.</p>

<p>RDD从原始的RDD穿件一个RDD的toples或者一个列表;<br/>
Step 1 被创建后, 创建 Schema 表示一个 StructType 匹配 RDD 中的结构.<br/>
通过 SparkSession 提供的 createDataFrame 方法应用 Schema 到 RDD .</p>

<pre><code>#For example:

# Import data types
from pyspark.sql.types import *

sc = spark.sparkContext

# Load a text file and convert each line to a Row.
lines = sc.textFile(&quot;examples/src/main/resources/people.txt&quot;)
parts = lines.map(lambda l: l.split(&quot;,&quot;))
# Each line is converted to a tuple.
people = parts.map(lambda p: (p[0], p[1].strip()))

# The schema is encoded in a string.
schemaString = &quot;name age&quot;

fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]
schema = StructType(fields)

# Apply the schema to the RDD.
schemaPeople = spark.createDataFrame(people, schema)

# Creates a temporary view using the DataFrame
schemaPeople.createOrReplaceTempView(&quot;people&quot;)

# SQL can be run over DataFrames that have been registered as a table.
results = spark.sql(&quot;SELECT name FROM people&quot;)

results.show()
# +-------+
# |   name|
# +-------+
# |Michael|
# |   Andy|
# | Justin|
# +-------+
</code></pre>

<p>Find full example code at &quot;examples/src/main/python/sql/basic.py&quot; in the Spark repo.</p>


		</div>

		

	</article>
  
	<div class="pagination">
	 <a class="prev" href="spark_1.html">&larr; Older</a> 
<a href="archives.html">Blog Archives</a>
	 
	    
	</div>
</div>
 <aside class="sidebar"> 

	<section>
	  <h1>Categories</h1>
	  <ul id="recent_posts">
	  
	      <li class="post">
	        <a href="%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6-%E6%B8%85%E5%8D%95.html"><strong>数据科学-清单&nbsp;(4)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="1%20Tools.html"><strong>1 Tools&nbsp;(33)</strong></a>
	         <p class="cat-children-p"> 
	        
	        	<a href="%E9%85%B7%E7%82%AB%E7%A5%9E%E5%99%A8.html">酷炫神器&nbsp;(12)</a>&nbsp;&nbsp;
	        
	        	<a href="PaddlePaddle.html">PaddlePaddle&nbsp;(3)</a>&nbsp;&nbsp;
	        
	        	<a href="spark.html">spark&nbsp;(12)</a>&nbsp;&nbsp;
	        
	        	<a href="tensorflow.html">tensorflow&nbsp;(2)</a>&nbsp;&nbsp;
	        
	        	<a href="SQL.html">SQL&nbsp;(1)</a>&nbsp;&nbsp;
	        
	         </p> 
	      </li>
	  
	      <li class="post">
	        <a href="2%20Get%20Data.html"><strong>2 Get Data&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="3%20%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>3 数据可视化&nbsp;(8)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="4%20%E4%BD%A0%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%9F%A5%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95.html"><strong>4 你不得不知的统计方法&nbsp;(9)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="5%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>5 机器学习&nbsp;(23)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.html"><strong>6 推荐系统&nbsp;(3)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="6%20%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86.html"><strong>6 图像处理&nbsp;(4)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="6%20%E6%90%9C%E7%B4%A2.html"><strong>6 搜索&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="6%20nlp.html"><strong>6 nlp&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="7%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html"><strong>7 深度学习&nbsp;(12)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="8%20Ai%E5%BA%94%E7%94%A8.html"><strong>8 Ai应用&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E8%AE%B8%E5%BC%8F%E4%BC%9F-%E6%9E%B6%E6%9E%84%E8%AF%BE.html"><strong>许式伟-架构课&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	   
	  </ul>
	</section>
	<section>
	  <h1>Recent Posts</h1>
	  <ul id="recent_posts">
	  
	      
		      <li class="post">
		        <a href="15633781017095.html"></a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15633691538884.html">3. 样本量的确定？</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15633655285542.html">卡方检验</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15632757602829.html">3. 假设检验的power</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15630063899159.html">python自然语言处理学习笔记——1</a>
		      </li>
	     
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	   
	  </ul>
	</section>
	
</aside> </div></div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  -
  <span class="credit">Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; Theme by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>

  
    


<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

</body>
</html>