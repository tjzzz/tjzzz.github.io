
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>
    
  spark - 学习笔记
  

  </title>
  <meta name="author" content="">
  <meta name="description" content="">

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link href="asset/css/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="atom.xml" rel="alternate" title="学习笔记" type="application/atom+xml">
  <script src="asset/js/modernizr-2.0.js"></script>
  <script src="asset/js/jquery.min.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/solarized_light.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>

  <style type="text/css">
  .cat-children-p{ padding: 6px 0px;}
  .hljs{background: none;}
  </style>
  <script type="text/javascript">
  var isAddSildbar = true;
  </script>
  <script src="asset/js/octopress.js" type="text/javascript"></script>
</head>
<script type="text/javascript">
//链接新开窗口
function addBlankTargetForLinks () {
  $('a[href^="http"]').each(function(){
      $(this).attr('target', '_blank');
  });
}
$(document).ready(function(event) {
  addBlankTargetForLinks();
});
</script>
<body   >
  <header role="banner"><hgroup>
  <h1><a href="index.html">学习笔记</a></h1>
  
    <h2></h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">

  <li id=""><a target="self" href="index.html">Home</a></li>

  <li id=""><a target="_self" href="archives.html">Archives</a></li>

</ul>

</nav>
  <div id="main">
    <div id="content"> 
<div class="blog-index">

	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15123123633940.html">study list</a></h1>
			<p class="meta"><time datetime="2017-12-03T22:46:03+08:00" 
			pubdate data-updated="true">2017/12/3</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p><a href="https://tracholar.github.io/wiki/#tools">https://tracholar.github.io/wiki/#tools</a></p>

<p>pyspark RDD<br/>
  <a href="http://www.jianshu.com/p/4cd22eda363f">http://www.jianshu.com/p/4cd22eda363f</a></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15123081634760.html">3 mL-特征提取</a></h1>
			<p class="meta"><time datetime="2017-12-03T21:36:03+08:00" 
			pubdate data-updated="true">2017/12/3</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p><a href="http://blog.csdn.net/cheng9981/article/details/63280665">http://blog.csdn.net/cheng9981/article/details/63280665</a></p>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15121083917876.html">2. ml/mllib相关的包</a></h1>
			<p class="meta"><time datetime="2017-12-01T14:06:31+08:00" 
			pubdate data-updated="true">2017/12/1</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<p>首先spark有两个关于机器学习的库ml和mllib.</p>

<ul>
<li><p>mllib<br/>
mllib是面向RDD的.目前官网上处于维护模式。</p>

<blockquote>
<p>The MLlib RDD-based API is now in maintenance mode。The primary Machine Learning API for Spark is now the DataFrame-based API in the spark.ml package.</p>
</blockquote></li>
<li><p>ml<br/>
ml的API是面向dataset的(dataframe是dataset的一个特例)。dataset的底端是RDD， dataset对RDD进行了优化，是更进一步的抽象。</p>

<ul>
<li>DataFrames 的许多好处包括 Spark Datasources，SQL/DataFrame 查询，Tungsten 和 Catalyst 优化以及跨语言的统一 API 。</li>
<li>用于 MLlib 的基于 DataFrame 的 API 为 ML algorithms （ML 算法）和跨多种语言提供了统一的 API 。</li>
<li>DataFrames 便于实际的 ML Pipelines （ML 管道），特别是 feature transformations （特征转换）。有关详细信息，请参阅 Pipelines 指南 。</li>
</ul></li>
</ul>

<p>关于两者的相似对比可以参看官方文档<br/>
<a href="http://spark.apache.org/docs/latest/ml-guide.html">http://spark.apache.org/docs/latest/ml-guide.html</a><br/>
<a href="http://spark.apachecn.org/docs/cn/2.2.0/ml-guide.html(%E4%B8%AD%E6%96%87)">http://spark.apachecn.org/docs/cn/2.2.0/ml-guide.html(中文)</a></p>

<p>参考资料：<a href="https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-practice5/index.html">https://www.ibm.com/developerworks/cn/opensource/os-cn-spark-practice5/index.html</a></p>

<h2 id="toc_0">【ml】Pipeline</h2>

<p>一个 Pipeline 在结构上会包含一个或多个 PipelineStage，每一个 PipelineStage 都会完成一个任务，如数据集处理转化，模型训练，参数设置或数据预测等，这样的 PipelineStage 在 ML 里按照处理问题类型的不同都有相应的定义和实现。首先需要了解几个主要的概念</p>

<ul>
<li>dataframe</li>
<li><p>transformer</p>

<ul>
<li><code>transform()</code>方法</li>
<li><strong>转换器</strong>，是一个 PipelineStage，实现上也是继承自 PipelineStage 类，主要是用来把 一个 DataFrame 转换成另一个 DataFrame，比如一个模型就是一个 Transformer，因为它可以把 一个不包含预测标签的测试数据集 DataFrame 打上标签转化成另一个包含预测标签的 DataFrame，显然这样的结果集可以被用来做分析结果的可视化。</li>
</ul></li>
<li><p>Estimator</p>

<ul>
<li>.fit()</li>
<li>Estimator 中文可以被翻译成评估器或适配器，在 Pipeline 里通常是被用来操作 DataFrame 数据并生产一个 Transformer，如一个随机森林算法就是一个 Estimator，因为它可以通过训练特征数据而得到一个随机森林模型。实现上 Estimator 也是继承自 PipelineStage 类</li>
</ul></li>
<li><p>Parameter</p>

<ul>
<li>Parameter 被用来设置 Transformer 或者 Estimator 的参数。</li>
</ul></li>
</ul>

<p>pipeline 就像是一个工作流</p>

<p><img src="media/15121083917876/15121120927254.jpg" alt=""/></p>

<h2 id="toc_1">【mllib】- DataTypes</h2>

<p>mllib支持读取libsvm格式的数据：</p>

<pre><code>label index1:value1 index2:value2 ..
</code></pre>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15116910127598.html">query文本聚类</a></h1>
			<p class="meta"><time datetime="2017-11-26T18:10:12+08:00" 
			pubdate data-updated="true">2017/11/26</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<ul>
<li class="task-list-item"><input disabled="disabled" type="checkbox" /> 是不是cache问题tf-idf
</li>
<li class="task-list-item"><input disabled="disabled" type="checkbox" /> 添加下降维的步骤
</li>
<li class="task-list-item"><input disabled="disabled" type="checkbox" /> 稀疏矩阵
</li>
<li class="task-list-item"><input disabled="disabled" type="checkbox" /> 数据切分更小li&#39;du
</li>
<li class="task-list-item"><input disabled="disabled" type="checkbox" /> Mini Batch KMeans <a href="http://www.dataivy.cn/blog/%E9%80%82%E5%90%88%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95mini-batch-k-means/">http://www.dataivy.cn/blog/%E9%80%82%E5%90%88%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95mini-batch-k-means/</a>
</li>
</ul>

<p>实验全量的<br/>
<a href="http://yq01-tianqi-spark-yarn00.yq01.baidu.com:8388/proxy/application_1511855101950_42074/">http://yq01-tianqi-spark-yarn00.yq01.baidu.com:8388/proxy/application_1511855101950_42074/</a></p>

<hr/>

<p>刚哥的代码：<a href="https://github.com/honestOrg/gxdgroup/blob/master/src/main/scala/cn/com/gxdgroup/dataplatform/avm/utils/AVMUtils.scala">https://github.com/honestOrg/gxdgroup/blob/master/src/main/scala/cn/com/gxdgroup/dataplatform/avm/utils/AVMUtils.scala</a></p>

<h2 id="toc_0">全量</h2>

<p>前期构造词频矩阵：</p>

<h2 id="toc_1">1.生成原始的词频矩阵</h2>

<ul>
<li><a href="http://spark.apache.org/docs/latest/ml-features.html#countvectorizer">http://spark.apache.org/docs/latest/ml-features.html#countvectorizer</a></li>
<li><code>examples/src/main/python/sql/basic.py</code></li>
</ul>

<p>输入数据：每行是切分好的term</p>

<pre><code>spark = SparkSession.builder.appName(&quot;CountVectorizerExample&quot;).getOrCreate()
sc = spark.sparkContext
# $example on$
# Input data: Each row is a bag of words with a ID.

lines = sc.textFile(&quot;/user/ubs/kce/zhenzhen/spark/test_data/corpus_title2&quot;)
parts = lines.map(lambda l: l.split(&#39;#&#39;))   # convert all the term to a list
termRDD = parts.map(lambda p: Row(tid=p[0], words=p[1].split(&#39; &#39;)))

df = spark.createDataFrame(termRDD)
# fit a CountVectorizerModel from the corpus.
cv = CountVectorizer(inputCol=&quot;words&quot;, outputCol=&quot;features&quot;)
#, vocabSize=3, minDF=2.0)

model = cv.fit(df)

result = model.transform(df)
</code></pre>

<h2 id="toc_2">2.tf-idf</h2>

<ul>
<li><a href="http://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf">http://spark.apache.org/docs/latest/mllib-feature-extraction.html#tf-idf</a></li>
<li><a href="http://spark.apache.org/docs/latest/ml-features.html#feature-extractors">http://spark.apache.org/docs/latest/ml-features.html#feature-extractors</a>
ml中的函数比mllib中的多
examples/src/main/python/mllib/tf_idf_example.py</li>
</ul>

<pre><code>~/zhenzhen/spark_study/spark/examples/src/main/python/mllib $  pyspark zzz_tf_idf_example.py
</code></pre>

<h2 id="toc_3">3.降维</h2>

<h2 id="toc_4">4.直接聚类</h2>

<pre><code> # pyspark这个默认是启动的集群任务，之前测试的本地数据会出错，脚本中需把输入目录改成集群路径
  ~/zhenzhen/spark_study $  pyspark kmeans.py  &gt; log.test
</code></pre>

<h2 id="toc_5">六神合体</h2>

<ul>
<li>词频矩阵+tfidf</li>
</ul>

<p><a href="http://www.zengyilun.com/spark-similarity/">http://www.zengyilun.com/spark-similarity/</a></p>

<p>上述所有步骤(tfidf，降维,聚类)合并在一起</p>

<pre><code> # pyspark这个默认是启动的集群任务，之前测试的本地数据会出错，脚本中需把输入目录改成集群路径
  ~/zhenzhen/spark_study $  pyspark tfidf_kmeans.py  &gt; log.test
  这个测试ok，但是中心点都是0，应该是数据问题
</code></pre>


		</div>

		

	</article>
 
	<article>
		 <header>
		  	<h1 class="entry-title"><a href="15116817738437.html">1.spark基础-RDD</a></h1>
			<p class="meta"><time datetime="2017-11-26T15:36:13+08:00" 
			pubdate data-updated="true">2017/11/26</time></p>
		 </header>
	  	<div class="entry-content">
		  	
		  	<h2 id="toc_0">1.RDD创建</h2>

<p>Spark是以RDD概念为中心运行的。RDD是一个容错的、可以被并行操作的元素集合。创建一个RDD有两个方法：在你的驱动程序中并行化一个已经存在的集合；从外部存储系统中引用一个数据集，这个存储系统可以是一个共享文件系统，比如HDFS、HBase或任意提供了Hadoop输入格式的数据来源。</p>

<p><strong>(1) RDD的创建—— 并行化集合</strong><br/>
并行化集合是通过在驱动程序中一个现有的迭代器或集合上调用SparkContext的parallelize方法建立的。为了创建一个能够并行操作的分布数据集，集合中的元素都会被拷贝</p>

<pre><code>data = [1, 2, 3, 4, 5]
distData = sc.parallelize(data)    #建立了分布数据集，可以进行一些并行的操作
</code></pre>

<p>并行化中可以自己设置数据集划分成分片的数量(一般是spark集群自动进行设定的)，比如<code>sc.parallelize(data, 10)</code></p>

<p><strong>(2)外部数据集</strong><br/>
PySpark可以通过Hadoop支持的外部数据源（包括本地文件系统、HDFS、 Cassandra、HBase、亚马逊S3等等）建立分布数据集。Spark支持文本文件、序列文件以及其他任何Hadoop输入格式文件.</p>

<p>（1）通过文本文件创建RDD要使用SparkContext的textfile方法</p>

<pre><code>from pyspark import SparkContext

if __name__ == &quot;__main__&quot;:
    sc = SparkContext(appName=&quot;zzz_KMeans&quot;)
    #调用文件的url/本地文件路径等
    lines = sc.textFile(&quot;/user/ubs/kce/zhenzhen/algorithm_analysis/query_classifier/quanliang/3C/term_matrix/&quot;)
</code></pre>

<p>注意</p>

<ul>
<li>包括textFile在内的所有基于文件的Spark读入方法，都支持将文件夹、压缩文件、包含通配符的路径作为参数</li>
<li>textFile方法也可以传入第二个可选参数来控制文件的分片数量。默认情况下，Spark会为文件的每一个块（在HDFS中块的大小默认是64MB）创建一个分片。但是你也可以通过传入一个更大的值来要求Spark建立更多的分片。注意，分片的数量绝不能小于文件块的数量。</li>
</ul>

<p><strong>(3)其他</strong></p>

<p>除了文本文件之外，pyspark还支持一些其他的数据格式</p>

<ul>
<li>SparkContext.wholeTextFiles能够读入包含多个小文本文件的目录，然后为每一个文件返回一个（文件名，内容)对。这是与textFile方法为每一个文本行返回一条记录相对应的。</li>
<li>RDD.saveAsPickleFile和SparkContext.pickleFile支持将RDD以串行化的Python对象格式存储起来。串行化的过程中会以默认10个一批的数量批量处理。</li>
<li>序列文件和其他Hadoop输入输出格式。</li>
</ul>

<p><strong>数据库</strong></p>

<h2 id="toc_1">2.RDD基本操作</h2>

<p>RDD的操作，整体上分为两类： <strong>转化操作和启动操作</strong>。</p>

<p><strong>转化操作</strong></p>

<ul>
<li>都是<code>惰性求值</code>的，就是说它们并不会立刻真的计算出结果。相反，它们仅仅是记录下了转换操作的操作对象（比如：一个文件）。只有当一个启动操作被执行，要向驱动程序返回结果时，转化操作才会真的开始计算</li>
<li>每一个由转化操作得到的RDD都会在每次执行启动操作时重新计算生成。也可以调用persist或者cache方法将RDD永久化到内存中。</li>
</ul>

<p><strong>启动操作</strong></p>

<p>常见的转化操作：</p>

<table>
<thead>
<tr>
<th>转化操作</th>
<th>作用</th>
</tr>
</thead>

<tbody>
<tr>
<td>map(func)</td>
<td>返回一个新的分布数据集，由原数据集元素经func处理后的结果组成</td>
</tr>
<tr>
<td>filter(func)</td>
<td>返回一个新的数据集，由传给func返回True的原数据集元素组成</td>
</tr>
<tr>
<td>flatMap(func)</td>
<td>与map类似，但是每个传入元素可能有0或多个返回值，func可以返回一个序列而不是一个值</td>
</tr>
<tr>
<td>mapParitions(func)</td>
<td>类似map，但是RDD的每个分片都会分开独立运行，所以func的参数和返回值必须都是迭代器</td>
</tr>
<tr>
<td>mapParitionsWithIndex(func)</td>
<td>类似mapParitions，但是func有两个参数，第一个是分片的序号，第二个是迭代器。返回值还是迭代器</td>
</tr>
<tr>
<td>sample(withReplacement, fraction, seed)</td>
<td>使用提供的随机数种子取样，然后替换或不替换</td>
</tr>
<tr>
<td>union(otherDataset)</td>
<td>返回新的数据集，包括原数据集和参数数据集的所有元素</td>
</tr>
<tr>
<td>intersection(otherDataset)</td>
<td>返回新数据集，是两个集的交集</td>
</tr>
<tr>
<td>distinct([numTasks])</td>
<td>返回新的集，包括原集中的不重复元素</td>
</tr>
<tr>
<td>groupByKey([numTasks])</td>
<td>当用于键值对RDD时返回(键，值迭代器)对的数据集</td>
</tr>
<tr>
<td>aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])</td>
<td>用于键值对RDD时返回（K，U）对集，对每一个Key的value进行聚集计算sortByKey([ascending], [numTasks])用于键值对RDD时会返回RDD按键的顺序排序，升降序由第一个参数决定</td>
</tr>
<tr>
<td>join(otherDataset, [numTasks])</td>
<td>用于键值对(K, V)和(K, W)RDD时返回(K, (V, W))对RDD</td>
</tr>
<tr>
<td>cogroup(otherDataset, [numTasks])</td>
<td>用于两个键值对RDD时返回(K, (V迭代器， W迭代器))RDD</td>
</tr>
<tr>
<td>cartesian(otherDataset)</td>
<td>用于T和U类型RDD时返回(T, U)对类型键值对RDD</td>
</tr>
<tr>
<td>pipe(command, [envVars])</td>
<td>通过shell命令管道处理每个RDD分片</td>
</tr>
<tr>
<td>coalesce(numPartitions)</td>
<td>把RDD的分片数量降低到参数大小</td>
</tr>
<tr>
<td>repartition(numPartitions)</td>
<td>重新打乱RDD中元素顺序并重新分片，数量由参数决定</td>
</tr>
<tr>
<td>repartitionAndSortWithinPartitions(partitioner)</td>
<td>按照参数给定的分片器重新分片，同时每个分片内部按照键排序</td>
</tr>
</tbody>
</table>

<p>常见的启动操作：</p>

<table>
<thead>
<tr>
<th>启动操作</th>
<th>作用</th>
</tr>
</thead>

<tbody>
<tr>
<td>reduce(func)</td>
<td>使用func进行聚集计算,func的参数是两个，返回值一个，两次func运行应当是完全解耦的，这样才能正确地并行运算</td>
</tr>
<tr>
<td>collect()</td>
<td>向驱动程序返回数据集的元素组成的数组</td>
</tr>
<tr>
<td>count()</td>
<td>返回数据集元素的数量</td>
</tr>
<tr>
<td>first()</td>
<td>返回数据集的第一个元素</td>
</tr>
<tr>
<td>take(n)</td>
<td>返回前n个元素组成的数组</td>
</tr>
<tr>
<td>takeSample(withReplacement, num, [seed])</td>
<td>返回一个由原数据集中任意num个元素的suzuki，并且替换之</td>
</tr>
<tr>
<td>takeOrder(n, [ordering])</td>
<td>返回排序后的前n个元素</td>
</tr>
<tr>
<td>saveAsTextFile(path)</td>
<td>将数据集的元素写成文本文件</td>
</tr>
<tr>
<td>saveAsSequenceFile(path)</td>
<td>将数据集的元素写成序列文件，这个API只能用于Java和Scala程序</td>
</tr>
<tr>
<td>saveAsObjectFile(path)</td>
<td>将数据集的元素使用Java的序列化特性写到文件中，这个API只能用于Java和Scala程序</td>
</tr>
<tr>
<td>countByCount()</td>
<td>只能用于键值对RDD，返回一个(K, int) hashmap，返回每个key的出现次数</td>
</tr>
<tr>
<td>foreach(func)</td>
<td>对数据集的每个元素执行func, 通常用于完成一些带有副作用的函数，比如更新累加器（见下文）或与外部存储交互等</td>
</tr>
</tbody>
</table>

<p><strong>RDD持久化</strong></p>

<p>主要用的两个方法<code>persist</code>和<code>cache</code></p>

<ul>
<li><p>Spark的一个重要功能就是在将数据集持久化（或缓存）到内存中以便在多个操作中重复使用。当我们持久化一个RDD是，每一个节点将这个RDD的每一个分片计算并保存到内存中以便在下次对这个数据集（或者这个数据集衍生的数据集）的计算中可以复用。这使得接下来的计算过程速度能够加快（经常能加快超过十倍的速度）.</p></li>
<li><p>每一个持久化的RDD都有一个可变的存储级别，这个级别使得用户可以改变RDD持久化的储存位置.</p></li>
<li><p>Spark会自动监视每个节点的缓存使用同时使用LRU算法丢弃旧数据分片。如果你想手动删除某个RDD而不是等待它被自动删除，调用<code>RDD.unpersist()</code>方法。</p></li>
</ul>

<p><strong>共享变量</strong></p>

<ul>
<li>广播变量</li>
<li>累加器</li>
</ul>

<h2 id="toc_2">3.RDD分区</h2>

<p>有时候需要重新设置Rdd的分区数量：</p>

<ul>
<li>比如Rdd的分区中，Rdd分区比较多，但是每个Rdd的数据量比较小，需要设置一个比较合理的分区。或者需要把Rdd的分区数量调大。</li>
<li>还有就是通过设置一个Rdd的分区来达到设置生成的文件的数量。</li>
</ul>

<p>有两种方法是可以重设Rdd的分区：分别是 coalesce()方法和repartition()</p>

<p>spark中的数据是分布式的</p>

<h2 id="toc_3">4.数据输出/保存</h2>

<p>rdd.saveAsTextFile()</p>

<p>【参考资料】</p>

<ul>
<li><p>详细的RDD的基础操作</p>

<p><a href="https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD">https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD</a></p></li>
<li><p><a href="http://cholerae.com/2015/04/11/-%E7%BF%BB%E8%AF%91-Spark%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97-Python%E7%89%88/">http://cholerae.com/2015/04/11/-%E7%BF%BB%E8%AF%91-Spark%E7%BC%96%E7%A8%8B%E6%8C%87%E5%8D%97-Python%E7%89%88/</a></p></li>
<li><p><a href="https://fangjian0423.github.io/2016/01/27/spark-programming-guide/">https://fangjian0423.github.io/2016/01/27/spark-programming-guide/</a></p></li>
</ul>


		</div>

		

	</article>
  
	<div class="pagination">
	 <a class="prev" href="spark_2.html">&larr; Older</a> 
<a href="archives.html">Blog Archives</a>
	 <a class="next" href="spark.html">Newer &rarr;</a>  
	    
	</div>
</div>
 <aside class="sidebar"> 

	<section>
	  <h1>Categories</h1>
	  <ul id="recent_posts">
	  
	      <li class="post">
	        <a href="%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6-%E6%B8%85%E5%8D%95.html"><strong>数据科学-清单&nbsp;(4)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="1%20Tools.html"><strong>1 Tools&nbsp;(33)</strong></a>
	         <p class="cat-children-p"> 
	        
	        	<a href="%E9%85%B7%E7%82%AB%E7%A5%9E%E5%99%A8.html">酷炫神器&nbsp;(12)</a>&nbsp;&nbsp;
	        
	        	<a href="PaddlePaddle.html">PaddlePaddle&nbsp;(3)</a>&nbsp;&nbsp;
	        
	        	<a href="spark.html">spark&nbsp;(12)</a>&nbsp;&nbsp;
	        
	        	<a href="tensorflow.html">tensorflow&nbsp;(2)</a>&nbsp;&nbsp;
	        
	        	<a href="SQL.html">SQL&nbsp;(1)</a>&nbsp;&nbsp;
	        
	         </p> 
	      </li>
	  
	      <li class="post">
	        <a href="2%20Get%20Data.html"><strong>2 Get Data&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="3%20%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96.html"><strong>3 数据可视化&nbsp;(8)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="4%20%E4%BD%A0%E4%B8%8D%E5%BE%97%E4%B8%8D%E7%9F%A5%E7%9A%84%E7%BB%9F%E8%AE%A1%E6%96%B9%E6%B3%95.html"><strong>4 你不得不知的统计方法&nbsp;(9)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="5%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html"><strong>5 机器学习&nbsp;(23)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.html"><strong>6 推荐系统&nbsp;(3)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="6%20%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86.html"><strong>6 图像处理&nbsp;(4)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="6%20%E6%90%9C%E7%B4%A2.html"><strong>6 搜索&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="6%20nlp.html"><strong>6 nlp&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="7%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0.html"><strong>7 深度学习&nbsp;(12)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="8%20Ai%E5%BA%94%E7%94%A8.html"><strong>8 Ai应用&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	  
	      <li class="post">
	        <a href="%E8%AE%B8%E5%BC%8F%E4%BC%9F-%E6%9E%B6%E6%9E%84%E8%AF%BE.html"><strong>许式伟-架构课&nbsp;(1)</strong></a>
	        
	        
	        
	      </li>
	   
	  </ul>
	</section>
	<section>
	  <h1>Recent Posts</h1>
	  <ul id="recent_posts">
	  
	      
		      <li class="post">
		        <a href="15633781017095.html"></a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15633691538884.html">3. 样本量的确定？</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15633655285542.html">卡方检验</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15632757602829.html">3. 假设检验的power</a>
		      </li>
	     
	  
	      
		      <li class="post">
		        <a href="15630063899159.html">python自然语言处理学习笔记——1</a>
		      </li>
	     
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	  
	      
	   
	  </ul>
	</section>
	
</aside> </div></div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2014 -  -
  <span class="credit">Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; Theme by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>

  
    


<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

</body>
</html>